{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778d5d6f-4787-4fe5-9e8e-c8b172b8904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a50eaf-0452-4893-aab1-f601fb4eb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697705ff-eba7-4b61-9d18-9bb72868a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64038ca-81bf-45dc-a76a-0ba0cafcf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.tasks.utils import evaluate, convert_to_nltk_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61924c3c-9f30-4a98-8be1-7948f633cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_process_strarcoder_generation(input_str):\n",
    "    pattern = r\"<EVALUATE>(.*?)</EVALUATE>\"\n",
    "    \n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, input_str, flags=re.DOTALL)\n",
    "    try:\n",
    "        return matches[8].strip()\n",
    "    except IndexError:\n",
    "        return input_str.split('<EVALUATE>')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8206d364-e0e6-4a83-b181-8903da1ceede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(generations, references, error_token):\n",
    "        correct = 0\n",
    "        for gens, ref in zip(generations, references):\n",
    "            gens = [gen for gen in gens if gen != error_token]\n",
    "            if len(gens) > 0:\n",
    "                majority = Counter(gens).most_common(1)[0][0]\n",
    "                if majority == ref:\n",
    "                    correct += 1\n",
    "        return {f\"accuracy (pass@1 majority)\": correct / len(references)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e73825-aa1a-4155-b45b-449cf5f7e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evals = []\n",
    "for generations in generations_raw:\n",
    "    sample_evals = []\n",
    "    for generation in generations:\n",
    "        all_propositions = [x.replace('FOL:','').strip()\n",
    "                            for x in generation.split('\\n')[1::2]]\n",
    "                            #for x in generation.split('<EVALUATE>')[9][:-12].strip().split('\\n')[1::2]]\n",
    "        premises, conclusion = all_propositions[:-1], all_propositions[-1]\n",
    "        try:\n",
    "            sample_evals.append(evaluate(premises, conclusion))\n",
    "        except:\n",
    "            sample_evals.append('Error')\n",
    "    all_evals.append(sample_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43764d-73ff-4e11-b4f3-988b10e5070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669a71b-9b2c-4896-afd4-99de1cea85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([(most_common(predictions),label) for predictions, label in zip(all_evals, references)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11743112-3111-4804-9403-d4ab7ef393fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([label in predictions for predictions, label in zip(all_evals, references)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23da7ed-b0d0-415c-b02a-fdb497510482",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_nltk_rep('∀x (Project(x) → (WrittenIn(x, cplusplus) ⊕ WrittenIn(x, python)))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161accd3-90df-4380-be58-43fb3a705b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('yale-nlp/FOLIO',split='train', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b32e11e-fb36-4294-b546-08a07509942e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m premises \u001b[38;5;241m=\u001b[39m [convert_to_nltk_rep(premise) \u001b[38;5;28;01mfor\u001b[39;00m premise \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremises-FOL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "premises = [convert_to_nltk_rep(premise) for premise in sample['premises-FOL'].split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7357d-431b-4db1-af65-5ac2d2e4e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion = convert_to_nltk_rep(sample['conclusion-FOL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0ceee7-68da-4886-947b-dc67bab83491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e46bc92934aedb1ed715395f3dc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unparsed_samples_xor = []\n",
    "unparsed_samples_other = []\n",
    "incorrect_samples = []\n",
    "correct_samples = []\n",
    "for sample in tqdm(dataset):\n",
    "    premises = [convert_to_nltk_rep(premise) for premise in sample['premises-FOL'].strip().split('\\n') if len(premise)]\n",
    "    conclusion = convert_to_nltk_rep(sample['conclusion-FOL'])\n",
    "    try:\n",
    "        result = evaluate(premises, conclusion)\n",
    "        if result != sample['label']:\n",
    "            incorrect_samples.append(sample)\n",
    "        else:\n",
    "            correct_samples.append(sample)\n",
    "    except:\n",
    "        if (''.join(premises)+conclusion).count('⊕'):\n",
    "            unparsed_samples_xor.append(sample)\n",
    "        else:\n",
    "            unparsed_samples_other.append(sample)\n",
    "        continue\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad363042-bb12-4fd9-95c4-845bb806a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(premises, conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc543c7-665a-4787-bbbc-4e943f6f67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"<PREMISES>\\n\"+sample['premises'].strip()+\"\\n</PREMISES>\\n<CONCLUSION>\\n\"+sample['conclusion'].strip()+\"\\n</CONCLUSION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "121b983f-e995-40b4-91eb-8ac2d8e187d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"The task is to translate each of the premises and conclusion into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\nExpressions should be adhere to the format of the Python NLTK package logic module.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd45dcc-9447-4334-bb87-ad1f9061e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "premises_text = sample['premises'].strip().split('\\n')\n",
    "premises_fol = [convert_to_nltk_rep(premise) for premise in sample['premises-FOL'].strip().split('\\n') if len(premise)]\n",
    "conclusion_text = sample['conclusion']\n",
    "conclusion_fol = convert_to_nltk_rep(sample['conclusion-FOL'])\n",
    "parsing_output = ('\\n'.join([f\"TEXT: {x}\\nFOL: {y}\" for (x,y) in zip(premises_text, premises_fol)])\n",
    "+f'\\nTEXT: {conclusion_text}\\nFOL: {conclusion_fol}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0e503f-3c6d-458e-9f44-5c6c82c7fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(sample, model='llama3-instruct', include_example=True):\n",
    "    if model=='llamacode':\n",
    "        system_message = \"\"\"The task is to translate each of the premises and conclusion into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\nExpressions should be adhere to the format of the Python NLTK package logic module.\"\"\"\n",
    "    elif model=='llama3-instruct':\n",
    "        system_message = f\"\"\"The task is to translate each of the premises and conclusion into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
    "Expressions should be adhere to the format of the Python NLTK package logic module:\n",
    "Conjunction (AND): A & B\n",
    "Disjunction (OR): A | B\n",
    "Implication: A -> B\n",
    "Negation: -A\n",
    "Universal Quantifier: all x. (proposition)\n",
    "Existential Quantifier: exists x. (proposition)\n",
    "\n",
    "Make sure that response is wrapped in EVALUATE tags.\"\"\"\n",
    "        if include_example:\n",
    "            system_message += ''' Follow the format of the provided example. \n",
    "<EXAMPLE>\n",
    "<EXAMPLE_INPUT>\n",
    "<PREMISES>\n",
    "Lawton Park is a neighborhood in Seattle. \n",
    "All citizens of Lawton Park use the zip code 98199. \n",
    "Tom is a citizen of Lawton Park.\n",
    "Daniel uses the zip code 98199.\n",
    "</PREMISES>\n",
    "<CONCLUSION>\n",
    "Tom is a citizen of Washington.\n",
    "</CONCLUSION>\n",
    "</EXAMPLE_INPUT>\n",
    "<EXAMPLE_OUTPUT>\n",
    "<EVALUATE>\n",
    "TEXT: Lawton Park is a neighborhood in Seattle. \n",
    "FOL: NeighbourhoodIn(LawtonPark, Seattle)\n",
    "TEXT: All citizens of Lawton Park use the zip code 98199. \n",
    "FOL: all x. (Residentof(x, LawtonPark) -> UseZipCode(x, NumNineEightOneNineNine))\n",
    "TEXT: Tom is a citizen of Lawton Park.\n",
    "FOL: ResidentOf(Tom, LawtonPark)\n",
    "TEXT: Daniel uses the zip code 98199.\n",
    "FOL: UseZipCode(Daniel, NumNineEightOneNineNine)\n",
    "TEXT: Tom is a citizen of Washington.\n",
    "FOL: ResidentOf(Tom, Washington)\n",
    "</EVALUATE>\n",
    "</EXAMPLE_OUTPUT>\n",
    "</EXAMPLE>'''\n",
    "    prompt = (\"<PREMISES>\\n\"+sample['premises'].strip()+\"\\n</PREMISES>\\n<CONCLUSION>\\n\"+sample['conclusion'].strip()+\"\\n</CONCLUSION>\")\n",
    "    premises_text = sample['premises'].strip().split('\\n')\n",
    "    premises_fol = [convert_to_nltk_rep(premise) for premise in sample['premises-FOL'].strip().split('\\n') if len(premise)]\n",
    "    conclusion_text = sample['conclusion']\n",
    "    conclusion_fol = convert_to_nltk_rep(sample['conclusion-FOL'])\n",
    "    parsing_output = ('\\n'.join([f\"TEXT: {x}\\nFOL: {y}\" for (x,y) in zip(premises_text, premises_fol)])\n",
    "    +f'\\nTEXT: {conclusion_text}\\nFOL: {conclusion_fol}')\n",
    "    if model=='llamacode':\n",
    "        formatted_sample=f\"\"\"Source: system\n",
    "{system_message}<step> Source: user\n",
    "\n",
    "{prompt} <step> Source: assistant\n",
    "Destination: user\n",
    "\n",
    "<EVALUATE>\n",
    "{parsing_output}\n",
    "</EVALUATE>\n",
    "\"\"\"\n",
    "    elif model=='llama3-instruct':\n",
    "        formatted_sample = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{{{ {system_message} }}}}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{{{ {prompt} }}}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{{{ <EVALUATE>\\n{parsing_output}\\n</EVALUATE> }}}}<|eot_id|><|end_of_text|>\"\"\"\n",
    "    else:\n",
    "        raise RuntimeError('Prompt wrapping is not implemented for this kind of model')\n",
    "    \n",
    "    return formatted_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1325e3d-c924-47a3-b5af-514601889ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SAMPLES=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4aebb7-355e-49ad-aa55-6df9f133d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatted_samples_train = '\\n\\n\\n'.join([format_sample(sample) for sample in correct_samples[:-VAL_SAMPLES]])\n",
    "concatted_samples_val= '\\n\\n\\n'.join([format_sample(sample) for sample in correct_samples[-VAL_SAMPLES:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b873bd20-038a-422d-b193-b6903b4722cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story_id': 136,\n",
       " 'premises': \"Phoneix's music is classified under the indie pop genre.\\nPhoenix is a band from France.\\nFrench bands write songs in French or in English.\\nAside from indie pop, pop rock and synth-pop are two other genres of music.\\nPhoenix has no songs in French.\",\n",
       " 'premises-FOL': 'IndiePop(phoenix)\\nBand(phoenix) ∧ From(phoenix, france)\\n∀x ∃y (Band(x) ∧ From(x, france) ∧ Write(x, y) ∧ Song(y) → InFrench(y) ⊕ InEnglish(y))\\n∀x (IndiePop(x) → ¬PopRock(x) ∧ ¬SynthPop(x))\\n∀x (Song(x) ∧ By(phoenix, x) → ¬InFrench(x))',\n",
       " 'conclusion': 'Phoenix writes songs in French.',\n",
       " 'conclusion-FOL': '∃x (Write(phoenix, y) ∧ Song(x) → InFrench(x))',\n",
       " 'label': 'False',\n",
       " 'example_id': 401}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c4563-5125-46c8-88d9-cdcef4b4f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "val_list = []\n",
    "for sample in correct_samples[:-VAL_SAMPLES]:\n",
    "    formatted_sample = format_sample(sample)\n",
    "    sample['text'] = formatted_sample\n",
    "    train_list.append(sample)\n",
    "\n",
    "for sample in correct_samples[-VAL_SAMPLES:]:\n",
    "    formatted_sample = format_sample(sample)\n",
    "    sample['text'] = formatted_sample\n",
    "    val_list.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2e1fff-0c97-480d-84e3-242de4bb5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_dataset('yale-nlp/FOLIO',split='validation', use_auth_token=True)\n",
    "test_list = []\n",
    "for sample in dataset_test:\n",
    "    formatted_sample = format_sample(sample)\n",
    "    sample['text'] = formatted_sample\n",
    "    test_list.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0dda8-8ad2-4ce8-9fbb-97d0e9932241",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('folio_filtered_train_llama3-instruct.json', 'w') as f:\n",
    "    json.dump(train_list, f)\n",
    "\n",
    "with open('folio_filtered_val_llama3-instruct.json', 'w') as f:\n",
    "    json.dump(val_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03a2d73-6157-478d-a80b-612517526d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('folio_filtered_test_llama3-instruct.json', 'w') as f:\n",
    "    json.dump(test_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40311ecf-bf4f-4c58-b74d-71d79e2dfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "{{ The task is to translate each of the premises and conclusion into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
      "Expressions should be adhere to the format of the Python NLTK package logic module:\n",
      "Conjunction (AND): A & B\n",
      "Disjunction (OR): A | B\n",
      "Implication: A -> B\n",
      "Negation: -A\n",
      "Universal Quantifier: all x. (proposition)\n",
      "Existential Quantifier: exists x. (proposition)\n",
      "\n",
      "Make sure that response is wrapped in EVALUATE tags. Follow the format of the provided example. \n",
      "<EXAMPLE>\n",
      "<EXAMPLE_INPUT>\n",
      "<PREMISES>\n",
      "Lawton Park is a neighborhood in Seattle. \n",
      "All citizens of Lawton Park use the zip code 98199. \n",
      "Tom is a citizen of Lawton Park.\n",
      "Daniel uses the zip code 98199.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "Tom is a citizen of Washington.\n",
      "</CONCLUSION>\n",
      "</EXAMPLE_INPUT>\n",
      "<EXAMPLE_OUTPUT>\n",
      "<EVALUATE>\n",
      "TEXT: Lawton Park is a neighborhood in Seattle. \n",
      "FOL: NeighbourhoodIn(LawtonPark, Seattle)\n",
      "TEXT: All citizens of Lawton Park use the zip code 98199. \n",
      "FOL: all x. (Residentof(x, LawtonPark) -> UseZipCode(x, NumNineEightOneNineNine))\n",
      "TEXT: Tom is a citizen of Lawton Park.\n",
      "FOL: ResidentOf(Tom, LawtonPark)\n",
      "TEXT: Daniel uses the zip code 98199.\n",
      "FOL: UseZipCode(Daniel, NumNineEightOneNineNine)\n",
      "TEXT: Tom is a citizen of Washington.\n",
      "FOL: ResidentOf(Tom, Washington)\n",
      "</EVALUATE>\n",
      "</EXAMPLE_OUTPUT>\n",
      "</EXAMPLE> }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "{{ <PREMISES>\n",
      "Any convicted criminal that is innocent is not truly guilty.\n",
      "All convicted criminals who did not commit a crime are truly innocent.\n",
      "All convicted criminals are truly guilty or found guilty.\n",
      "If a convicted criminal is found guilty, then they are sentenced to a punishment.\n",
      "If a convicted criminal is found guilty, then they can argue against their punishment.\n",
      "Garry is a convicted criminal who not found guilty or is sentenced to punishment.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "Garry is not both innocent and someone who did not commit a crime.\n",
      "</CONCLUSION> }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ <EVALUATE>\n",
      "TEXT: Any convicted criminal that is innocent is not truly guilty.\n",
      "FOL: all x. (ConvictedCriminal(x) & Innocent(x) -> -TrulyGuilty(x))\n",
      "TEXT: All convicted criminals who did not commit a crime are truly innocent.\n",
      "FOL: all x. (ConvictedCriminal(x) & -CommitCrime(x) -> Innocent(x))\n",
      "TEXT: All convicted criminals are truly guilty or found guilty.\n",
      "FOL: all x. (ConvictedCriminal(x) & (TrulyGuilty(x) | FoundGuilty(x)))\n",
      "TEXT: If a convicted criminal is found guilty, then they are sentenced to a punishment.\n",
      "FOL: all x. (ConvictedCriminal(x) & FoundGuilty(x) -> SentencedToPunishment(x))\n",
      "TEXT: If a convicted criminal is found guilty, then they can argue against their punishment.\n",
      "FOL: all x. (ConvictedCriminal(x) & FoundGuilty(x) -> CanArgueAgainst(x, Punishment))\n",
      "TEXT: Garry is a convicted criminal who not found guilty or is sentenced to punishment.\n",
      "FOL: ConvictedCriminal(Garry) & (-(FoundGuilty(Garry) | SentencedToPunishment(Garry)))\n",
      "TEXT: Garry is not both innocent and someone who did not commit a crime.\n",
      "FOL: -(Innocent(Garry) & -CommitCrime(Garry))\n",
      "</EVALUATE> }}<|eot_id|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(format_sample(correct_samples[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c3e9a-e4c0-4ce7-834c-d5bbbefb9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/root/text-generation-webui/training/datasets/folio_filtered_train_llama3-instruct.txt', 'w') as f:\n",
    "#     f.write(concatted_samples_train)\n",
    "\n",
    "# with open('/root/text-generation-webui/training/datasets/folio_filtered_val_llama3-instruct.txt', 'w') as f:\n",
    "#     f.write(concatted_samples_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec86cd7-0863-4e8d-982e-8dea56bded42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myale-nlp/FOLIO\u001b[39m\u001b[38;5;124m'\u001b[39m,split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_val = load_dataset('yale-nlp/FOLIO',split='validation', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e8fb88-1011-4ca1-936f-c22e2c5f39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_val in dataset_val:\n",
    "    if sample_val['example_id']==0:\n",
    "        break\n",
    "#sample_val = dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cd33a52-efe0-4367-b127-575fcbff3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_local_llm(sample, model='llama3-instruct'):\n",
    "    if model == 'llamacode':\n",
    "        prompt = format_sample(sample).split('<EVALUATE>')[0]\n",
    "        stop_strings = ['<step>']\n",
    "    elif model == 'llama3-instruct':\n",
    "        prompt = format_sample(sample).split('<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[0]\n",
    "        prompt += '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{{ '\n",
    "        stop_strings = ['}}assistant']\n",
    "    else:\n",
    "        raise RuntimeError('Unknown model prompt wrapping')\n",
    "    url = \"http://127.0.0.1:5000/v1/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 1.05,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stopping_strings\": stop_strings,\n",
    "        \"stop\": stop_strings,\n",
    "        \"ban_eos_token\":True\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    generation = json.loads(response.text)['choices'][0]['text']\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d678b660-7136-4e48-99b0-ffd63aa90e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(generation):\n",
    "    try:\n",
    "        all_propositions = [x.replace('FOL:','').strip()\n",
    "                                for x in generation.split('\\n')[2:-1:2]]\n",
    "                                #for x in generation.split('<EVALUATE>')[9][:-12].strip().split('\\n')[1::2]]\n",
    "        premises, conclusion = all_propositions[:-1], all_propositions[-1]\n",
    "        return evaluate(premises, conclusion)\n",
    "    except:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719d9363-4607-4fe7-94d4-bac7296370f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = correct_samples[0]\n",
    "for sample in correct_samples:\n",
    "    pattern = r\"<EVALUATE>(.*?)</EVALUATE>\"\n",
    "    input_str = format_sample(sample)\n",
    "    matches = re.findall(pattern, input_str, flags=re.DOTALL)\n",
    "    assert evaluate_generation(matches[-1])==sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90d81970-13cd-41f4-8efb-642f83486414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_generation('''<EVALUATE>\n",
    "TEXT: All eels are fish. \n",
    "FOL: all x. (Eel(x) -> Fish(x))\n",
    "TEXT: No fish are plants. \n",
    "FOL: all x. (Fish(x) -> -Plant(x))\n",
    "TEXT: Everything displayed in the collection is either a plant or an animal.\n",
    "FOL: all x. (DisplayedIn(x, Collection) -> ((Plant(x) & -Animal(x)) | (-Plant(x) & Animal(x))))\n",
    "TEXT: All multicellular animals are not bacteria.\n",
    "FOL: all x. (Multicellular(x) -> -Bacteria(x))\n",
    "TEXT: All animals displayed in the collection are multicellular.\n",
    "FOL: all x. (DisplayedIn(x, Collection) & Animal(x) -> Multicellular(x))\n",
    "TEXT: A sea eel is displayed in the collection.\n",
    "FOL: DisplayedIn(SeaEel, Collection)\n",
    "TEXT: The sea eel is an eel or an animal or not a plant.\n",
    "FOL: Eel(SeaEel) | Animal(SeaEel) | -Plant(SeaEel)\n",
    "TEXT: The sea eel is bacteria.\n",
    "FOL: Bacteria(SeaEel)\n",
    "</EVALUATE>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b946ac6-be98-496a-bf19-d9517c59126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall(pattern, input_str, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6b73578-acb7-4c3d-9fef-fabc2013d862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09103170aad42349d0630a01d9d2932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: Uncertain, prediction: Uncertain\n",
      "Ground truth: True, prediction: Error\n",
      "Ground truth: Uncertain, prediction: Error\n",
      "Ground truth: Uncertain, prediction: Uncertain\n",
      "Ground truth: True, prediction: Error\n",
      "Ground truth: False, prediction: Error\n",
      "Ground truth: Uncertain, prediction: Uncertain\n",
      "Ground truth: True, prediction: Uncertain\n",
      "Ground truth: Uncertain, prediction: Uncertain\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(correct_samples[\u001b[38;5;241m-\u001b[39mVAL_SAMPLES:]):\n\u001b[0;32m----> 4\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[43mquery_local_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m evaluate_generation(generation)\n\u001b[1;32m      6\u001b[0m     gt \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36mquery_local_llm\u001b[0;34m(sample, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mban_eos_token\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m generation \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gts = []\n",
    "predictions = []\n",
    "for sample in tqdm(correct_samples[-VAL_SAMPLES:]):\n",
    "    generation = query_local_llm(sample)\n",
    "    prediction = evaluate_generation(generation)\n",
    "    gt = sample['label']\n",
    "    gts.append(gt)\n",
    "    predictions.append(prediction)\n",
    "    print(f'Ground truth: {gt}, prediction: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adecccb-4b96-494a-8150-3b36e51c89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fc60e-9a95-444b-9b61-9b87e99f832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x==y  for x,y in zip(gts, predictions)])/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922e4da-c898-4417-b429-937a20fb50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x==y and x!= 'Uncertain' for x,y in zip(gts, predictions)])/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc5412-a809-4e1f-a397-7b3a3a99c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca07135-e021-43a3-add6-fc1472979731",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_samples[-4]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae5324-ccba-4974-b728-10b850531d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_generation(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ddfce-986d-42f6-8c4e-d0f108257b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in tqdm(dataset):\n",
    "    for attempt in range(8):\n",
    "        generation = query_local_llm(sample)\n",
    "        result = evaluate_generation(generation)\n",
    "        with open(f'/root/codellama_samples/example_id_{sample[\"example_id\"]}_attempt_{attempt}_gt_{sample[\"label\"]}_pred_{result}.txt', 'w') as f:\n",
    "            f.write(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d57762c-14f7-43df-b94d-80c826f9ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAL_FOLDER = '/root/codellama_samples_val_zaebal_v2/'\n",
    "#VAL_FOLDER = '/root/llama3_raw_val/'\n",
    "VAL_FOLDER = '/root/codellama_samples_val_optuna/'\n",
    "#os.makedirs(VAL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04a761-8f84-484d-abe1-f25d678a7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in tqdm(dataset_val):\n",
    "    for attempt in range(8):\n",
    "        generation = query_local_llm(sample)\n",
    "        try:\n",
    "            result = evaluate_generation(generation)\n",
    "        except:\n",
    "            result = \"Error\"\n",
    "            print(f'Evaluation failure for example_id_{sample[\"example_id\"]}_attempt_{attempt}_gt_{sample[\"label\"]}')\n",
    "        with open(os.path.join(VAL_FOLDER, f'example_id_{sample[\"example_id\"]}_attempt_{attempt}_gt_{sample[\"label\"]}_pred_{result}.txt'), 'w') as f:\n",
    "            f.write(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab8d1fc-02f0-4ad1-af09-b06346fe3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fps = os.listdir('/root/codellama_samples_val/')\n",
    "#fps = os.listdir('/root/another_instance_copy/codellama_samples_val_dpo/codellama_samples_val_dpo/')\n",
    "fps = os.listdir(VAL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "964674b3-7cdf-47d6-87a2-b90965872a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "569c5287-5f93-4f19-9e6e-acf1784dd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb408b7-245c-4c8b-8b70-c1611ede29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results  = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed7e46c-cb58-4492-b6ff-656d1178323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abc02bce-de33-4948-b541-951e3c895d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "for fp in fps:\n",
    "    fp_splitted = os.path.splitext(fp)[0].split('_')\n",
    "    if len(fp_splitted)!=9:\n",
    "        print('Skipping', fp)\n",
    "        continue\n",
    "    id, gt, prediction = fp_splitted[2], fp_splitted[6], fp_splitted[8]\n",
    "    gts[id] = gt\n",
    "    results[id].append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c8618a0-fa81-49f7-a32e-ed1b58b370ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916256157635468"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([gts[key] in results[key] for key in gts])/len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85a54ef-e3b6-4aa9-aef9-dda39ed3a0eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'most_common_exlcuding_errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m([gts[key]\u001b[38;5;241m==\u001b[39mmost_common_exlcuding_errors(results[key]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m gts])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(gts)\n",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m([gts[key]\u001b[38;5;241m==\u001b[39m\u001b[43mmost_common_exlcuding_errors\u001b[49m(results[key]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m gts])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(gts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'most_common_exlcuding_errors' is not defined"
     ]
    }
   ],
   "source": [
    "sum([gts[key]==most_common_exlcuding_errors(results[key]) for key in gts])/len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "825e9195-b2e4-44fc-a84f-64f6417f8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [gts[key] for key in gts]\n",
    "results_lst = [results[key] for key in gts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51923217-0643-4716-b76b-08b5d038ece4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy (pass@1 majority)': 0.7832512315270936}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(results_lst, references, \"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd54f2-add5-462f-8807-425f47e218e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = os.listdir('/root/codellama_samples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a96c2-1497-40ea-9fe5-74925d71501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = {}\n",
    "results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e3e29-503a-4cdb-a8a2-1c2197d736be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in fps:\n",
    "    fp_splitted = os.path.splitext(fp)[0].split('_')\n",
    "    if len(fp_splitted)!=9:\n",
    "        print('Skipping', fp)\n",
    "        continue\n",
    "    id, gt, prediction = fp_splitted[2], fp_splitted[6], fp_splitted[8]\n",
    "    gts[id] = gt\n",
    "    results[id].append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d315e2-7815-479e-92fe-5f432101ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preference_pairs = 0\n",
    "not_diverse_ids = []\n",
    "for id in results:\n",
    "    incorrect_preds = len([x for x in results[id] if x!=gts[id]])\n",
    "    sample_pairs = incorrect_preds*(len(results[id])-incorrect_preds)\n",
    "    num_preference_pairs += sample_pairs\n",
    "    if sample_pairs==0:\n",
    "        not_diverse_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b2385-85ff-40f7-b132-b2fb18090469",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('yale-nlp/FOLIO',split='train', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04fa55-1406-44fc-bbca-20b192d8b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_diverse_samples = [sample for sample in dataset if str(sample['example_id']) in not_diverse_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863d146-098a-4d5c-8393-bcd69d4213a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/root/codellama_samples_addition/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ddf5b-743a-42d6-bb10-d0b8afea4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in tqdm(not_diverse_samples):\n",
    "    for attempt in range(8):\n",
    "        generation = query_local_llm(sample)\n",
    "        try:\n",
    "            result = evaluate_generation(generation)\n",
    "        except:\n",
    "            result = \"Error\"\n",
    "            print(f'Evaluation failure for example_id_{sample[\"example_id\"]}_attempt_{attempt}_gt_{sample[\"label\"]}')\n",
    "        with open(f'/root/codellama_samples_addition/example_id_{sample[\"example_id\"]}_attempt_{attempt}_gt_{sample[\"label\"]}_pred_{result}.txt', 'w') as f:\n",
    "            f.write(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403dce85-dbf0-41e1-a6f4-c82ad9363024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14225134-00bc-45f8-ba45-49aa126c1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('unsloth/llama-3-70b-Instruct-bnb-4bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fc956-cd21-4b33-8435-bef7cdb79ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "(pd.Series([len(tokenizer(format_sample(sample))['input_ids']) for sample in dataset])<800).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa28094-a661-41e4-a3c3-fd4b00008aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(format_sample(sample))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6d7dc-6949-4f4f-baea-0e0034883e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer(format_sample(sample)[:512])['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
