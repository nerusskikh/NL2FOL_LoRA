{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc141027-996d-4ab9-b595-6916eee51ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  6 22:50:21 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   34C    P3              55W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15484503-99f7-48e6-92d0-9912cc25808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
    "#from unsloth import is_bfloat16_supported\n",
    "PatchDPOTrainer()\n",
    "import torch\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from trl import DPOTrainer\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c00920-2167-4c5e-ae42-47467c330c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_FP = \"/root/text-generation-webui/models/unsloth_llama-3-70b-Instruct-bnb-4bit/\"\n",
    "TRAIN_DATA_FP = 'data/llama3-instruct_train_dpo.json'\n",
    "QLORA_FP = '/root/text-generation-webui/loras/37outof50_with_2_errors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9a3aac-e7fb-4625-b92e-82f28b86c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024+256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823b0097-b4dc-46a4-8b65-72c8dcd8ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_json(TRAIN_DATA_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1924ef8e-0136-479f-9ed5-3de13ab48a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA RTX 6000 Ada Generation. Max memory: 47.507 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa34a7751def4af8935504bb53080f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/text-generation-webui/models/unsloth_llama-3-70b-Instruct-bnb-4bit/ does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n",
      "Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2024.4 patched 80 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(model_name=QLORA_FP, use_gradient_checkpointing = \"unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb6b673-4ee5-4a59-8e60-646f28ff164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "\n",
    "\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     base_model,\n",
    "#     r = 32,\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0, # Dropout = 0 is currently optimized\n",
    "#     bias = \"none\",    # Bias = \"none\" is currently optimized\n",
    "#     use_gradient_checkpointing = True,\n",
    "#     random_state = 3407,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433cf032-d0a8-429f-a36c-3cd61686ccb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.32.self_attn.k_proj.weight', 'base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.32.self_attn.o_proj.weight', 'base_model.model.model.layers.32.mlp.gate_proj.weight', 'base_model.model.model.layers.32.mlp.up_proj.weight', 'base_model.model.model.layers.32.mlp.down_proj.weight', 'base_model.model.model.layers.32.input_layernorm.weight', 'base_model.model.model.layers.32.post_attention_layernorm.weight', 'base_model.model.model.layers.33.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.33.self_attn.k_proj.weight', 'base_model.model.model.layers.33.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.33.self_attn.o_proj.weight', 'base_model.model.model.layers.33.mlp.gate_proj.weight', 'base_model.model.model.layers.33.mlp.up_proj.weight', 'base_model.model.model.layers.33.mlp.down_proj.weight', 'base_model.model.model.layers.33.input_layernorm.weight', 'base_model.model.model.layers.33.post_attention_layernorm.weight', 'base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.34.self_attn.k_proj.weight', 'base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.34.self_attn.o_proj.weight', 'base_model.model.model.layers.34.mlp.gate_proj.weight', 'base_model.model.model.layers.34.mlp.up_proj.weight', 'base_model.model.model.layers.34.mlp.down_proj.weight', 'base_model.model.model.layers.34.input_layernorm.weight', 'base_model.model.model.layers.34.post_attention_layernorm.weight', 'base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.35.self_attn.k_proj.weight', 'base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.35.self_attn.o_proj.weight', 'base_model.model.model.layers.35.mlp.gate_proj.weight', 'base_model.model.model.layers.35.mlp.up_proj.weight', 'base_model.model.model.layers.35.mlp.down_proj.weight', 'base_model.model.model.layers.35.input_layernorm.weight', 'base_model.model.model.layers.35.post_attention_layernorm.weight', 'base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.36.self_attn.k_proj.weight', 'base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.36.self_attn.o_proj.weight', 'base_model.model.model.layers.36.mlp.gate_proj.weight', 'base_model.model.model.layers.36.mlp.up_proj.weight', 'base_model.model.model.layers.36.mlp.down_proj.weight', 'base_model.model.model.layers.36.input_layernorm.weight', 'base_model.model.model.layers.36.post_attention_layernorm.weight', 'base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.37.self_attn.k_proj.weight', 'base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.37.self_attn.o_proj.weight', 'base_model.model.model.layers.37.mlp.gate_proj.weight', 'base_model.model.model.layers.37.mlp.up_proj.weight', 'base_model.model.model.layers.37.mlp.down_proj.weight', 'base_model.model.model.layers.37.input_layernorm.weight', 'base_model.model.model.layers.37.post_attention_layernorm.weight', 'base_model.model.model.layers.38.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.38.self_attn.k_proj.weight', 'base_model.model.model.layers.38.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.38.self_attn.o_proj.weight', 'base_model.model.model.layers.38.mlp.gate_proj.weight', 'base_model.model.model.layers.38.mlp.up_proj.weight', 'base_model.model.model.layers.38.mlp.down_proj.weight', 'base_model.model.model.layers.38.input_layernorm.weight', 'base_model.model.model.layers.38.post_attention_layernorm.weight', 'base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.39.self_attn.k_proj.weight', 'base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.39.self_attn.o_proj.weight', 'base_model.model.model.layers.39.mlp.gate_proj.weight', 'base_model.model.model.layers.39.mlp.up_proj.weight', 'base_model.model.model.layers.39.mlp.down_proj.weight', 'base_model.model.model.layers.39.input_layernorm.weight', 'base_model.model.model.layers.39.post_attention_layernorm.weight', 'base_model.model.model.layers.40.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.40.self_attn.k_proj.weight', 'base_model.model.model.layers.40.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.40.self_attn.o_proj.weight', 'base_model.model.model.layers.40.mlp.gate_proj.weight', 'base_model.model.model.layers.40.mlp.up_proj.weight', 'base_model.model.model.layers.40.mlp.down_proj.weight', 'base_model.model.model.layers.40.input_layernorm.weight', 'base_model.model.model.layers.40.post_attention_layernorm.weight', 'base_model.model.model.layers.41.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.41.self_attn.k_proj.weight', 'base_model.model.model.layers.41.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.41.self_attn.o_proj.weight', 'base_model.model.model.layers.41.mlp.gate_proj.weight', 'base_model.model.model.layers.41.mlp.up_proj.weight', 'base_model.model.model.layers.41.mlp.down_proj.weight', 'base_model.model.model.layers.41.input_layernorm.weight', 'base_model.model.model.layers.41.post_attention_layernorm.weight', 'base_model.model.model.layers.42.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.42.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.42.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.42.self_attn.k_proj.weight', 'base_model.model.model.layers.42.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.42.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.42.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.42.self_attn.o_proj.weight', 'base_model.model.model.layers.42.mlp.gate_proj.weight', 'base_model.model.model.layers.42.mlp.up_proj.weight', 'base_model.model.model.layers.42.mlp.down_proj.weight', 'base_model.model.model.layers.42.input_layernorm.weight', 'base_model.model.model.layers.42.post_attention_layernorm.weight', 'base_model.model.model.layers.43.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.43.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.43.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.43.self_attn.k_proj.weight', 'base_model.model.model.layers.43.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.43.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.43.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.43.self_attn.o_proj.weight', 'base_model.model.model.layers.43.mlp.gate_proj.weight', 'base_model.model.model.layers.43.mlp.up_proj.weight', 'base_model.model.model.layers.43.mlp.down_proj.weight', 'base_model.model.model.layers.43.input_layernorm.weight', 'base_model.model.model.layers.43.post_attention_layernorm.weight', 'base_model.model.model.layers.44.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.44.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.44.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.44.self_attn.k_proj.weight', 'base_model.model.model.layers.44.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.44.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.44.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.44.self_attn.o_proj.weight', 'base_model.model.model.layers.44.mlp.gate_proj.weight', 'base_model.model.model.layers.44.mlp.up_proj.weight', 'base_model.model.model.layers.44.mlp.down_proj.weight', 'base_model.model.model.layers.44.input_layernorm.weight', 'base_model.model.model.layers.44.post_attention_layernorm.weight', 'base_model.model.model.layers.45.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.45.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.45.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.45.self_attn.k_proj.weight', 'base_model.model.model.layers.45.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.45.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.45.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.45.self_attn.o_proj.weight', 'base_model.model.model.layers.45.mlp.gate_proj.weight', 'base_model.model.model.layers.45.mlp.up_proj.weight', 'base_model.model.model.layers.45.mlp.down_proj.weight', 'base_model.model.model.layers.45.input_layernorm.weight', 'base_model.model.model.layers.45.post_attention_layernorm.weight', 'base_model.model.model.layers.46.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.46.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.46.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.46.self_attn.k_proj.weight', 'base_model.model.model.layers.46.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.46.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.46.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.46.self_attn.o_proj.weight', 'base_model.model.model.layers.46.mlp.gate_proj.weight', 'base_model.model.model.layers.46.mlp.up_proj.weight', 'base_model.model.model.layers.46.mlp.down_proj.weight', 'base_model.model.model.layers.46.input_layernorm.weight', 'base_model.model.model.layers.46.post_attention_layernorm.weight', 'base_model.model.model.layers.47.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.47.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.47.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.47.self_attn.k_proj.weight', 'base_model.model.model.layers.47.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.47.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.47.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.47.self_attn.o_proj.weight', 'base_model.model.model.layers.47.mlp.gate_proj.weight', 'base_model.model.model.layers.47.mlp.up_proj.weight', 'base_model.model.model.layers.47.mlp.down_proj.weight', 'base_model.model.model.layers.47.input_layernorm.weight', 'base_model.model.model.layers.47.post_attention_layernorm.weight', 'base_model.model.model.layers.48.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.48.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.48.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.48.self_attn.k_proj.weight', 'base_model.model.model.layers.48.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.48.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.48.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.48.self_attn.o_proj.weight', 'base_model.model.model.layers.48.mlp.gate_proj.weight', 'base_model.model.model.layers.48.mlp.up_proj.weight', 'base_model.model.model.layers.48.mlp.down_proj.weight', 'base_model.model.model.layers.48.input_layernorm.weight', 'base_model.model.model.layers.48.post_attention_layernorm.weight', 'base_model.model.model.layers.49.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.49.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.49.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.49.self_attn.k_proj.weight', 'base_model.model.model.layers.49.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.49.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.49.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.49.self_attn.o_proj.weight', 'base_model.model.model.layers.49.mlp.gate_proj.weight', 'base_model.model.model.layers.49.mlp.up_proj.weight', 'base_model.model.model.layers.49.mlp.down_proj.weight', 'base_model.model.model.layers.49.input_layernorm.weight', 'base_model.model.model.layers.49.post_attention_layernorm.weight', 'base_model.model.model.layers.50.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.50.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.50.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.50.self_attn.k_proj.weight', 'base_model.model.model.layers.50.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.50.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.50.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.50.self_attn.o_proj.weight', 'base_model.model.model.layers.50.mlp.gate_proj.weight', 'base_model.model.model.layers.50.mlp.up_proj.weight', 'base_model.model.model.layers.50.mlp.down_proj.weight', 'base_model.model.model.layers.50.input_layernorm.weight', 'base_model.model.model.layers.50.post_attention_layernorm.weight', 'base_model.model.model.layers.51.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.51.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.51.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.51.self_attn.k_proj.weight', 'base_model.model.model.layers.51.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.51.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.51.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.51.self_attn.o_proj.weight', 'base_model.model.model.layers.51.mlp.gate_proj.weight', 'base_model.model.model.layers.51.mlp.up_proj.weight', 'base_model.model.model.layers.51.mlp.down_proj.weight', 'base_model.model.model.layers.51.input_layernorm.weight', 'base_model.model.model.layers.51.post_attention_layernorm.weight', 'base_model.model.model.layers.52.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.52.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.52.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.52.self_attn.k_proj.weight', 'base_model.model.model.layers.52.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.52.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.52.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.52.self_attn.o_proj.weight', 'base_model.model.model.layers.52.mlp.gate_proj.weight', 'base_model.model.model.layers.52.mlp.up_proj.weight', 'base_model.model.model.layers.52.mlp.down_proj.weight', 'base_model.model.model.layers.52.input_layernorm.weight', 'base_model.model.model.layers.52.post_attention_layernorm.weight', 'base_model.model.model.layers.53.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.53.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.53.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.53.self_attn.k_proj.weight', 'base_model.model.model.layers.53.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.53.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.53.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.53.self_attn.o_proj.weight', 'base_model.model.model.layers.53.mlp.gate_proj.weight', 'base_model.model.model.layers.53.mlp.up_proj.weight', 'base_model.model.model.layers.53.mlp.down_proj.weight', 'base_model.model.model.layers.53.input_layernorm.weight', 'base_model.model.model.layers.53.post_attention_layernorm.weight', 'base_model.model.model.layers.54.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.54.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.54.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.54.self_attn.k_proj.weight', 'base_model.model.model.layers.54.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.54.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.54.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.54.self_attn.o_proj.weight', 'base_model.model.model.layers.54.mlp.gate_proj.weight', 'base_model.model.model.layers.54.mlp.up_proj.weight', 'base_model.model.model.layers.54.mlp.down_proj.weight', 'base_model.model.model.layers.54.input_layernorm.weight', 'base_model.model.model.layers.54.post_attention_layernorm.weight', 'base_model.model.model.layers.55.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.55.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.55.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.55.self_attn.k_proj.weight', 'base_model.model.model.layers.55.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.55.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.55.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.55.self_attn.o_proj.weight', 'base_model.model.model.layers.55.mlp.gate_proj.weight', 'base_model.model.model.layers.55.mlp.up_proj.weight', 'base_model.model.model.layers.55.mlp.down_proj.weight', 'base_model.model.model.layers.55.input_layernorm.weight', 'base_model.model.model.layers.55.post_attention_layernorm.weight', 'base_model.model.model.layers.56.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.56.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.56.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.56.self_attn.k_proj.weight', 'base_model.model.model.layers.56.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.56.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.56.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.56.self_attn.o_proj.weight', 'base_model.model.model.layers.56.mlp.gate_proj.weight', 'base_model.model.model.layers.56.mlp.up_proj.weight', 'base_model.model.model.layers.56.mlp.down_proj.weight', 'base_model.model.model.layers.56.input_layernorm.weight', 'base_model.model.model.layers.56.post_attention_layernorm.weight', 'base_model.model.model.layers.57.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.57.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.57.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.57.self_attn.k_proj.weight', 'base_model.model.model.layers.57.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.57.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.57.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.57.self_attn.o_proj.weight', 'base_model.model.model.layers.57.mlp.gate_proj.weight', 'base_model.model.model.layers.57.mlp.up_proj.weight', 'base_model.model.model.layers.57.mlp.down_proj.weight', 'base_model.model.model.layers.57.input_layernorm.weight', 'base_model.model.model.layers.57.post_attention_layernorm.weight', 'base_model.model.model.layers.58.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.58.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.58.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.58.self_attn.k_proj.weight', 'base_model.model.model.layers.58.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.58.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.58.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.58.self_attn.o_proj.weight', 'base_model.model.model.layers.58.mlp.gate_proj.weight', 'base_model.model.model.layers.58.mlp.up_proj.weight', 'base_model.model.model.layers.58.mlp.down_proj.weight', 'base_model.model.model.layers.58.input_layernorm.weight', 'base_model.model.model.layers.58.post_attention_layernorm.weight', 'base_model.model.model.layers.59.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.59.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.59.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.59.self_attn.k_proj.weight', 'base_model.model.model.layers.59.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.59.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.59.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.59.self_attn.o_proj.weight', 'base_model.model.model.layers.59.mlp.gate_proj.weight', 'base_model.model.model.layers.59.mlp.up_proj.weight', 'base_model.model.model.layers.59.mlp.down_proj.weight', 'base_model.model.model.layers.59.input_layernorm.weight', 'base_model.model.model.layers.59.post_attention_layernorm.weight', 'base_model.model.model.layers.60.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.60.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.60.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.60.self_attn.k_proj.weight', 'base_model.model.model.layers.60.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.60.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.60.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.60.self_attn.o_proj.weight', 'base_model.model.model.layers.60.mlp.gate_proj.weight', 'base_model.model.model.layers.60.mlp.up_proj.weight', 'base_model.model.model.layers.60.mlp.down_proj.weight', 'base_model.model.model.layers.60.input_layernorm.weight', 'base_model.model.model.layers.60.post_attention_layernorm.weight', 'base_model.model.model.layers.61.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.61.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.61.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.61.self_attn.k_proj.weight', 'base_model.model.model.layers.61.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.61.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.61.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.61.self_attn.o_proj.weight', 'base_model.model.model.layers.61.mlp.gate_proj.weight', 'base_model.model.model.layers.61.mlp.up_proj.weight', 'base_model.model.model.layers.61.mlp.down_proj.weight', 'base_model.model.model.layers.61.input_layernorm.weight', 'base_model.model.model.layers.61.post_attention_layernorm.weight', 'base_model.model.model.layers.62.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.62.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.62.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.62.self_attn.k_proj.weight', 'base_model.model.model.layers.62.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.62.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.62.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.62.self_attn.o_proj.weight', 'base_model.model.model.layers.62.mlp.gate_proj.weight', 'base_model.model.model.layers.62.mlp.up_proj.weight', 'base_model.model.model.layers.62.mlp.down_proj.weight', 'base_model.model.model.layers.62.input_layernorm.weight', 'base_model.model.model.layers.62.post_attention_layernorm.weight', 'base_model.model.model.layers.63.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.63.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.63.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.63.self_attn.k_proj.weight', 'base_model.model.model.layers.63.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.63.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.63.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.63.self_attn.o_proj.weight', 'base_model.model.model.layers.63.mlp.gate_proj.weight', 'base_model.model.model.layers.63.mlp.up_proj.weight', 'base_model.model.model.layers.63.mlp.down_proj.weight', 'base_model.model.model.layers.63.input_layernorm.weight', 'base_model.model.model.layers.63.post_attention_layernorm.weight', 'base_model.model.model.layers.64.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.64.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.64.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.64.self_attn.k_proj.weight', 'base_model.model.model.layers.64.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.64.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.64.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.64.self_attn.o_proj.weight', 'base_model.model.model.layers.64.mlp.gate_proj.weight', 'base_model.model.model.layers.64.mlp.up_proj.weight', 'base_model.model.model.layers.64.mlp.down_proj.weight', 'base_model.model.model.layers.64.input_layernorm.weight', 'base_model.model.model.layers.64.post_attention_layernorm.weight', 'base_model.model.model.layers.65.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.65.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.65.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.65.self_attn.k_proj.weight', 'base_model.model.model.layers.65.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.65.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.65.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.65.self_attn.o_proj.weight', 'base_model.model.model.layers.65.mlp.gate_proj.weight', 'base_model.model.model.layers.65.mlp.up_proj.weight', 'base_model.model.model.layers.65.mlp.down_proj.weight', 'base_model.model.model.layers.65.input_layernorm.weight', 'base_model.model.model.layers.65.post_attention_layernorm.weight', 'base_model.model.model.layers.66.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.66.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.66.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.66.self_attn.k_proj.weight', 'base_model.model.model.layers.66.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.66.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.66.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.66.self_attn.o_proj.weight', 'base_model.model.model.layers.66.mlp.gate_proj.weight', 'base_model.model.model.layers.66.mlp.up_proj.weight', 'base_model.model.model.layers.66.mlp.down_proj.weight', 'base_model.model.model.layers.66.input_layernorm.weight', 'base_model.model.model.layers.66.post_attention_layernorm.weight', 'base_model.model.model.layers.67.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.67.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.67.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.67.self_attn.k_proj.weight', 'base_model.model.model.layers.67.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.67.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.67.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.67.self_attn.o_proj.weight', 'base_model.model.model.layers.67.mlp.gate_proj.weight', 'base_model.model.model.layers.67.mlp.up_proj.weight', 'base_model.model.model.layers.67.mlp.down_proj.weight', 'base_model.model.model.layers.67.input_layernorm.weight', 'base_model.model.model.layers.67.post_attention_layernorm.weight', 'base_model.model.model.layers.68.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.68.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.68.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.68.self_attn.k_proj.weight', 'base_model.model.model.layers.68.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.68.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.68.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.68.self_attn.o_proj.weight', 'base_model.model.model.layers.68.mlp.gate_proj.weight', 'base_model.model.model.layers.68.mlp.up_proj.weight', 'base_model.model.model.layers.68.mlp.down_proj.weight', 'base_model.model.model.layers.68.input_layernorm.weight', 'base_model.model.model.layers.68.post_attention_layernorm.weight', 'base_model.model.model.layers.69.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.69.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.69.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.69.self_attn.k_proj.weight', 'base_model.model.model.layers.69.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.69.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.69.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.69.self_attn.o_proj.weight', 'base_model.model.model.layers.69.mlp.gate_proj.weight', 'base_model.model.model.layers.69.mlp.up_proj.weight', 'base_model.model.model.layers.69.mlp.down_proj.weight', 'base_model.model.model.layers.69.input_layernorm.weight', 'base_model.model.model.layers.69.post_attention_layernorm.weight', 'base_model.model.model.layers.70.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.70.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.70.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.70.self_attn.k_proj.weight', 'base_model.model.model.layers.70.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.70.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.70.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.70.self_attn.o_proj.weight', 'base_model.model.model.layers.70.mlp.gate_proj.weight', 'base_model.model.model.layers.70.mlp.up_proj.weight', 'base_model.model.model.layers.70.mlp.down_proj.weight', 'base_model.model.model.layers.70.input_layernorm.weight', 'base_model.model.model.layers.70.post_attention_layernorm.weight', 'base_model.model.model.layers.71.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.71.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.71.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.71.self_attn.k_proj.weight', 'base_model.model.model.layers.71.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.71.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.71.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.71.self_attn.o_proj.weight', 'base_model.model.model.layers.71.mlp.gate_proj.weight', 'base_model.model.model.layers.71.mlp.up_proj.weight', 'base_model.model.model.layers.71.mlp.down_proj.weight', 'base_model.model.model.layers.71.input_layernorm.weight', 'base_model.model.model.layers.71.post_attention_layernorm.weight', 'base_model.model.model.layers.72.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.72.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.72.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.72.self_attn.k_proj.weight', 'base_model.model.model.layers.72.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.72.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.72.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.72.self_attn.o_proj.weight', 'base_model.model.model.layers.72.mlp.gate_proj.weight', 'base_model.model.model.layers.72.mlp.up_proj.weight', 'base_model.model.model.layers.72.mlp.down_proj.weight', 'base_model.model.model.layers.72.input_layernorm.weight', 'base_model.model.model.layers.72.post_attention_layernorm.weight', 'base_model.model.model.layers.73.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.73.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.73.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.73.self_attn.k_proj.weight', 'base_model.model.model.layers.73.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.73.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.73.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.73.self_attn.o_proj.weight', 'base_model.model.model.layers.73.mlp.gate_proj.weight', 'base_model.model.model.layers.73.mlp.up_proj.weight', 'base_model.model.model.layers.73.mlp.down_proj.weight', 'base_model.model.model.layers.73.input_layernorm.weight', 'base_model.model.model.layers.73.post_attention_layernorm.weight', 'base_model.model.model.layers.74.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.74.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.74.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.74.self_attn.k_proj.weight', 'base_model.model.model.layers.74.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.74.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.74.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.74.self_attn.o_proj.weight', 'base_model.model.model.layers.74.mlp.gate_proj.weight', 'base_model.model.model.layers.74.mlp.up_proj.weight', 'base_model.model.model.layers.74.mlp.down_proj.weight', 'base_model.model.model.layers.74.input_layernorm.weight', 'base_model.model.model.layers.74.post_attention_layernorm.weight', 'base_model.model.model.layers.75.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.75.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.75.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.75.self_attn.k_proj.weight', 'base_model.model.model.layers.75.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.75.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.75.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.75.self_attn.o_proj.weight', 'base_model.model.model.layers.75.mlp.gate_proj.weight', 'base_model.model.model.layers.75.mlp.up_proj.weight', 'base_model.model.model.layers.75.mlp.down_proj.weight', 'base_model.model.model.layers.75.input_layernorm.weight', 'base_model.model.model.layers.75.post_attention_layernorm.weight', 'base_model.model.model.layers.76.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.76.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.76.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.76.self_attn.k_proj.weight', 'base_model.model.model.layers.76.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.76.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.76.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.76.self_attn.o_proj.weight', 'base_model.model.model.layers.76.mlp.gate_proj.weight', 'base_model.model.model.layers.76.mlp.up_proj.weight', 'base_model.model.model.layers.76.mlp.down_proj.weight', 'base_model.model.model.layers.76.input_layernorm.weight', 'base_model.model.model.layers.76.post_attention_layernorm.weight', 'base_model.model.model.layers.77.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.77.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.77.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.77.self_attn.k_proj.weight', 'base_model.model.model.layers.77.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.77.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.77.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.77.self_attn.o_proj.weight', 'base_model.model.model.layers.77.mlp.gate_proj.weight', 'base_model.model.model.layers.77.mlp.up_proj.weight', 'base_model.model.model.layers.77.mlp.down_proj.weight', 'base_model.model.model.layers.77.input_layernorm.weight', 'base_model.model.model.layers.77.post_attention_layernorm.weight', 'base_model.model.model.layers.78.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.78.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.78.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.78.self_attn.k_proj.weight', 'base_model.model.model.layers.78.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.78.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.78.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.78.self_attn.o_proj.weight', 'base_model.model.model.layers.78.mlp.gate_proj.weight', 'base_model.model.model.layers.78.mlp.up_proj.weight', 'base_model.model.model.layers.78.mlp.down_proj.weight', 'base_model.model.model.layers.78.input_layernorm.weight', 'base_model.model.model.layers.78.post_attention_layernorm.weight', 'base_model.model.model.layers.79.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.79.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.79.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.79.self_attn.k_proj.weight', 'base_model.model.model.layers.79.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.79.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.79.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.79.self_attn.o_proj.weight', 'base_model.model.model.layers.79.mlp.gate_proj.weight', 'base_model.model.model.layers.79.mlp.up_proj.weight', 'base_model.model.model.layers.79.mlp.down_proj.weight', 'base_model.model.model.layers.79.input_layernorm.weight', 'base_model.model.model.layers.79.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter(QLORA_FP, adapter_name='reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "030b9615-1408-4acd-9ecd-84193a9ddf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7859f18-7fda-4120-9704-1f7b026aa580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adapter a second time, with a different name, which will be our reference model.\n",
    "#model.load_adapter(QLORA_FP, adapter_name=\"reference\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52448bc7-efc6-49a3-ab73-c4edf96d6990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 19., 134., 452., 569., 423., 280., 255., 111.,  34.,  15.]),\n",
       " array([ 525. ,  589.2,  653.4,  717.6,  781.8,  846. ,  910.2,  974.4,\n",
       "        1038.6, 1102.8, 1167. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(tokenizer.encode(x['prompt']))+max(len(tokenizer.encode(x['rejected'])), len(tokenizer.encode(x['chosen']))) for x in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af2d64f2-0fc8-4e7e-8a37-099b603ee354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 72., 498., 816., 450., 233., 174.,  34.,   0.,   0.,  15.]),\n",
       " array([436. , 457.9, 479.8, 501.7, 523.6, 545.5, 567.4, 589.3, 611.2,\n",
       "        633.1, 655. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(tokenizer.encode(x['prompt'])) for x in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbea6dd-c2ef-45dc-8309-5415ba73eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,292 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 8 | Total steps = 858\n",
      " \"-____-\"     Number of trainable parameters = 262,144,000\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='197' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [197/858 3:33:26 < 12:03:30, 0.02 it/s, Epoch 0.68/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-27.142984</td>\n",
       "      <td>-26.198196</td>\n",
       "      <td>0.211682</td>\n",
       "      <td>0.209226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-29.517939</td>\n",
       "      <td>-26.686356</td>\n",
       "      <td>0.236194</td>\n",
       "      <td>0.242418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>-0.008891</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.014057</td>\n",
       "      <td>-33.295029</td>\n",
       "      <td>-24.746006</td>\n",
       "      <td>0.183436</td>\n",
       "      <td>0.172751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>-0.007116</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.014595</td>\n",
       "      <td>-27.891937</td>\n",
       "      <td>-15.872861</td>\n",
       "      <td>0.215611</td>\n",
       "      <td>0.224054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>-0.113025</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.129615</td>\n",
       "      <td>-31.094555</td>\n",
       "      <td>-21.981188</td>\n",
       "      <td>0.231036</td>\n",
       "      <td>0.232821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>-0.116079</td>\n",
       "      <td>-0.066079</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-28.000820</td>\n",
       "      <td>-36.585037</td>\n",
       "      <td>0.196637</td>\n",
       "      <td>0.194673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>-0.022360</td>\n",
       "      <td>-0.052115</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.029755</td>\n",
       "      <td>-31.201651</td>\n",
       "      <td>-24.451223</td>\n",
       "      <td>0.222151</td>\n",
       "      <td>0.224939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>-0.028498</td>\n",
       "      <td>-0.034903</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>-27.214504</td>\n",
       "      <td>-16.304646</td>\n",
       "      <td>0.189833</td>\n",
       "      <td>0.188696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.624200</td>\n",
       "      <td>-0.039975</td>\n",
       "      <td>-0.209909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.169934</td>\n",
       "      <td>-29.671440</td>\n",
       "      <td>-14.655938</td>\n",
       "      <td>0.234351</td>\n",
       "      <td>0.232474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>-0.076314</td>\n",
       "      <td>-0.224592</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.148278</td>\n",
       "      <td>-23.686352</td>\n",
       "      <td>-23.105507</td>\n",
       "      <td>0.236654</td>\n",
       "      <td>0.237471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>-0.276651</td>\n",
       "      <td>-0.469952</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.193301</td>\n",
       "      <td>-25.460979</td>\n",
       "      <td>-18.884062</td>\n",
       "      <td>0.235226</td>\n",
       "      <td>0.232775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>-0.410281</td>\n",
       "      <td>-0.933188</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>-43.978130</td>\n",
       "      <td>-29.071636</td>\n",
       "      <td>0.234395</td>\n",
       "      <td>0.235240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>-0.421254</td>\n",
       "      <td>-1.057657</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636402</td>\n",
       "      <td>-33.951237</td>\n",
       "      <td>-21.279526</td>\n",
       "      <td>0.240132</td>\n",
       "      <td>0.245836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.617600</td>\n",
       "      <td>-0.724860</td>\n",
       "      <td>-0.945091</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.220232</td>\n",
       "      <td>-32.578773</td>\n",
       "      <td>-25.002897</td>\n",
       "      <td>0.216592</td>\n",
       "      <td>0.200150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>-1.212040</td>\n",
       "      <td>-1.534156</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.322116</td>\n",
       "      <td>-37.973316</td>\n",
       "      <td>-34.812080</td>\n",
       "      <td>0.220928</td>\n",
       "      <td>0.230801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>-1.851907</td>\n",
       "      <td>-2.289137</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.437230</td>\n",
       "      <td>-49.845516</td>\n",
       "      <td>-40.020645</td>\n",
       "      <td>0.218482</td>\n",
       "      <td>0.219964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>-2.317658</td>\n",
       "      <td>-2.258303</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.059355</td>\n",
       "      <td>-44.857819</td>\n",
       "      <td>-43.125725</td>\n",
       "      <td>0.237762</td>\n",
       "      <td>0.238526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>-0.605470</td>\n",
       "      <td>-1.117610</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.512140</td>\n",
       "      <td>-37.405384</td>\n",
       "      <td>-34.241177</td>\n",
       "      <td>0.225330</td>\n",
       "      <td>0.225675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>-0.613020</td>\n",
       "      <td>-1.295221</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.682201</td>\n",
       "      <td>-46.036774</td>\n",
       "      <td>-25.070087</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.213760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>-1.337694</td>\n",
       "      <td>-1.676086</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.338392</td>\n",
       "      <td>-47.426292</td>\n",
       "      <td>-39.073780</td>\n",
       "      <td>0.224721</td>\n",
       "      <td>0.229996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>-1.826538</td>\n",
       "      <td>-1.975163</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.148626</td>\n",
       "      <td>-52.964676</td>\n",
       "      <td>-40.248062</td>\n",
       "      <td>0.226749</td>\n",
       "      <td>0.244829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>-1.436158</td>\n",
       "      <td>-1.752420</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.316263</td>\n",
       "      <td>-45.056717</td>\n",
       "      <td>-42.193485</td>\n",
       "      <td>0.230552</td>\n",
       "      <td>0.239141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>-0.857642</td>\n",
       "      <td>-1.876326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.018684</td>\n",
       "      <td>-50.141891</td>\n",
       "      <td>-33.442692</td>\n",
       "      <td>0.224242</td>\n",
       "      <td>0.249425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>-0.889774</td>\n",
       "      <td>-1.410050</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.520276</td>\n",
       "      <td>-34.345398</td>\n",
       "      <td>-23.741356</td>\n",
       "      <td>0.248299</td>\n",
       "      <td>0.248776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>-1.385521</td>\n",
       "      <td>-1.771578</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.386057</td>\n",
       "      <td>-52.291653</td>\n",
       "      <td>-34.382313</td>\n",
       "      <td>0.219467</td>\n",
       "      <td>0.217298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>-3.120596</td>\n",
       "      <td>-3.457526</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.336930</td>\n",
       "      <td>-62.433289</td>\n",
       "      <td>-52.030441</td>\n",
       "      <td>0.225232</td>\n",
       "      <td>0.229902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.797600</td>\n",
       "      <td>-2.009511</td>\n",
       "      <td>-2.038530</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.029019</td>\n",
       "      <td>-44.036861</td>\n",
       "      <td>-42.079517</td>\n",
       "      <td>0.213590</td>\n",
       "      <td>0.212196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>-2.008291</td>\n",
       "      <td>-2.800387</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.792096</td>\n",
       "      <td>-60.092464</td>\n",
       "      <td>-45.347076</td>\n",
       "      <td>0.220439</td>\n",
       "      <td>0.255364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.731100</td>\n",
       "      <td>-2.188463</td>\n",
       "      <td>-2.510823</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.322360</td>\n",
       "      <td>-51.868279</td>\n",
       "      <td>-47.330147</td>\n",
       "      <td>0.254447</td>\n",
       "      <td>0.264816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>-1.484651</td>\n",
       "      <td>-2.434941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950289</td>\n",
       "      <td>-52.107361</td>\n",
       "      <td>-33.276810</td>\n",
       "      <td>0.207985</td>\n",
       "      <td>0.245808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>-2.304316</td>\n",
       "      <td>-3.910285</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.605969</td>\n",
       "      <td>-66.376816</td>\n",
       "      <td>-44.610199</td>\n",
       "      <td>0.276462</td>\n",
       "      <td>0.313563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>-2.171546</td>\n",
       "      <td>-3.645311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.473765</td>\n",
       "      <td>-66.932106</td>\n",
       "      <td>-42.282570</td>\n",
       "      <td>0.232913</td>\n",
       "      <td>0.223723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>-5.190728</td>\n",
       "      <td>-6.117488</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.926759</td>\n",
       "      <td>-89.814316</td>\n",
       "      <td>-74.088028</td>\n",
       "      <td>0.267338</td>\n",
       "      <td>0.298087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>-5.786131</td>\n",
       "      <td>-5.928166</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142036</td>\n",
       "      <td>-81.554939</td>\n",
       "      <td>-78.113503</td>\n",
       "      <td>0.250532</td>\n",
       "      <td>0.257388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>-5.186288</td>\n",
       "      <td>-6.549886</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.363598</td>\n",
       "      <td>-91.175835</td>\n",
       "      <td>-74.132668</td>\n",
       "      <td>0.290983</td>\n",
       "      <td>0.299621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>-2.307003</td>\n",
       "      <td>-2.813753</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.506750</td>\n",
       "      <td>-50.761829</td>\n",
       "      <td>-41.572399</td>\n",
       "      <td>0.235192</td>\n",
       "      <td>0.252851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>-2.558887</td>\n",
       "      <td>-2.826228</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.267341</td>\n",
       "      <td>-48.374931</td>\n",
       "      <td>-49.176098</td>\n",
       "      <td>0.189788</td>\n",
       "      <td>0.192551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>-2.438434</td>\n",
       "      <td>-2.790941</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.352507</td>\n",
       "      <td>-55.477768</td>\n",
       "      <td>-49.427925</td>\n",
       "      <td>0.223230</td>\n",
       "      <td>0.247618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.654800</td>\n",
       "      <td>-2.769444</td>\n",
       "      <td>-3.268960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.499516</td>\n",
       "      <td>-62.559940</td>\n",
       "      <td>-51.284569</td>\n",
       "      <td>0.239731</td>\n",
       "      <td>0.256539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.868800</td>\n",
       "      <td>-2.370604</td>\n",
       "      <td>-2.576300</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.205695</td>\n",
       "      <td>-43.211571</td>\n",
       "      <td>-42.531273</td>\n",
       "      <td>0.241255</td>\n",
       "      <td>0.238813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>-2.275981</td>\n",
       "      <td>-3.427196</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.151215</td>\n",
       "      <td>-61.407707</td>\n",
       "      <td>-38.422432</td>\n",
       "      <td>0.219631</td>\n",
       "      <td>0.242670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>-2.985059</td>\n",
       "      <td>-3.222257</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.237197</td>\n",
       "      <td>-54.671703</td>\n",
       "      <td>-47.214470</td>\n",
       "      <td>0.206554</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.563600</td>\n",
       "      <td>-3.251479</td>\n",
       "      <td>-3.744054</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.492575</td>\n",
       "      <td>-62.498314</td>\n",
       "      <td>-53.250809</td>\n",
       "      <td>0.249155</td>\n",
       "      <td>0.219285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>-1.507881</td>\n",
       "      <td>-2.358814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850933</td>\n",
       "      <td>-53.162514</td>\n",
       "      <td>-35.850800</td>\n",
       "      <td>0.223492</td>\n",
       "      <td>0.216996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>-2.473657</td>\n",
       "      <td>-3.247680</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.774023</td>\n",
       "      <td>-55.114479</td>\n",
       "      <td>-40.733494</td>\n",
       "      <td>0.254115</td>\n",
       "      <td>0.259541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>-2.565220</td>\n",
       "      <td>-3.944605</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.379385</td>\n",
       "      <td>-71.035545</td>\n",
       "      <td>-52.426159</td>\n",
       "      <td>0.243344</td>\n",
       "      <td>0.257514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.582800</td>\n",
       "      <td>-2.396114</td>\n",
       "      <td>-3.015531</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.619417</td>\n",
       "      <td>-57.912304</td>\n",
       "      <td>-48.215969</td>\n",
       "      <td>0.221086</td>\n",
       "      <td>0.221160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>-3.854785</td>\n",
       "      <td>-4.642695</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.787910</td>\n",
       "      <td>-73.650269</td>\n",
       "      <td>-56.862312</td>\n",
       "      <td>0.232261</td>\n",
       "      <td>0.242909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>-4.734560</td>\n",
       "      <td>-5.891627</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.157068</td>\n",
       "      <td>-87.122307</td>\n",
       "      <td>-71.986740</td>\n",
       "      <td>0.231009</td>\n",
       "      <td>0.237115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>-1.841061</td>\n",
       "      <td>-3.145688</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.304627</td>\n",
       "      <td>-75.245117</td>\n",
       "      <td>-46.067776</td>\n",
       "      <td>0.192303</td>\n",
       "      <td>0.194561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>-2.048434</td>\n",
       "      <td>-3.973430</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.924996</td>\n",
       "      <td>-66.814819</td>\n",
       "      <td>-45.321285</td>\n",
       "      <td>0.155585</td>\n",
       "      <td>0.161187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>-2.030468</td>\n",
       "      <td>-3.305636</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.275169</td>\n",
       "      <td>-55.084579</td>\n",
       "      <td>-35.675831</td>\n",
       "      <td>0.158019</td>\n",
       "      <td>0.170718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>-2.503523</td>\n",
       "      <td>-4.065154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.561632</td>\n",
       "      <td>-68.368988</td>\n",
       "      <td>-46.380341</td>\n",
       "      <td>0.183501</td>\n",
       "      <td>0.182062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>-3.038373</td>\n",
       "      <td>-3.434408</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-61.225967</td>\n",
       "      <td>-53.218281</td>\n",
       "      <td>0.148003</td>\n",
       "      <td>0.150345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>-2.972955</td>\n",
       "      <td>-5.620380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.647425</td>\n",
       "      <td>-88.186485</td>\n",
       "      <td>-51.296379</td>\n",
       "      <td>0.172878</td>\n",
       "      <td>0.184835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>-3.131216</td>\n",
       "      <td>-5.912528</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.781312</td>\n",
       "      <td>-89.936157</td>\n",
       "      <td>-58.692871</td>\n",
       "      <td>0.180165</td>\n",
       "      <td>0.181986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>-3.496803</td>\n",
       "      <td>-4.539562</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.042760</td>\n",
       "      <td>-73.727310</td>\n",
       "      <td>-59.871902</td>\n",
       "      <td>0.168813</td>\n",
       "      <td>0.172056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>-3.082101</td>\n",
       "      <td>-5.783700</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.701599</td>\n",
       "      <td>-84.722794</td>\n",
       "      <td>-51.425308</td>\n",
       "      <td>0.186045</td>\n",
       "      <td>0.179927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>-5.201717</td>\n",
       "      <td>-5.660963</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.459245</td>\n",
       "      <td>-79.515953</td>\n",
       "      <td>-71.430267</td>\n",
       "      <td>0.138092</td>\n",
       "      <td>0.130474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>-2.376828</td>\n",
       "      <td>-4.471037</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.094209</td>\n",
       "      <td>-72.435272</td>\n",
       "      <td>-44.543930</td>\n",
       "      <td>0.150008</td>\n",
       "      <td>0.174548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.287100</td>\n",
       "      <td>-3.388577</td>\n",
       "      <td>-4.914811</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.526235</td>\n",
       "      <td>-74.199852</td>\n",
       "      <td>-51.810055</td>\n",
       "      <td>0.156485</td>\n",
       "      <td>0.167793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>-2.644868</td>\n",
       "      <td>-4.296194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.651325</td>\n",
       "      <td>-66.920937</td>\n",
       "      <td>-44.935951</td>\n",
       "      <td>0.159470</td>\n",
       "      <td>0.183820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.487800</td>\n",
       "      <td>-4.356310</td>\n",
       "      <td>-5.890110</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.533800</td>\n",
       "      <td>-82.941002</td>\n",
       "      <td>-67.540512</td>\n",
       "      <td>0.181992</td>\n",
       "      <td>0.204529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>-4.187957</td>\n",
       "      <td>-5.448087</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.260130</td>\n",
       "      <td>-86.876976</td>\n",
       "      <td>-71.224060</td>\n",
       "      <td>0.207442</td>\n",
       "      <td>0.211218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.758500</td>\n",
       "      <td>-3.671111</td>\n",
       "      <td>-5.229774</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.558663</td>\n",
       "      <td>-78.759483</td>\n",
       "      <td>-57.761295</td>\n",
       "      <td>0.160072</td>\n",
       "      <td>0.168031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>-3.056811</td>\n",
       "      <td>-4.587588</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.530777</td>\n",
       "      <td>-75.048698</td>\n",
       "      <td>-56.038712</td>\n",
       "      <td>0.204389</td>\n",
       "      <td>0.198215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>-2.608853</td>\n",
       "      <td>-3.959427</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.350574</td>\n",
       "      <td>-69.582718</td>\n",
       "      <td>-51.254646</td>\n",
       "      <td>0.162920</td>\n",
       "      <td>0.167224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.338000</td>\n",
       "      <td>-2.722104</td>\n",
       "      <td>-3.902903</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.180798</td>\n",
       "      <td>-70.053185</td>\n",
       "      <td>-57.511616</td>\n",
       "      <td>0.216642</td>\n",
       "      <td>0.217321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>-2.454344</td>\n",
       "      <td>-4.313665</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.859322</td>\n",
       "      <td>-70.304169</td>\n",
       "      <td>-43.631599</td>\n",
       "      <td>0.172854</td>\n",
       "      <td>0.188329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>-1.594974</td>\n",
       "      <td>-4.898152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.303178</td>\n",
       "      <td>-92.281998</td>\n",
       "      <td>-39.425789</td>\n",
       "      <td>0.250205</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>-2.035192</td>\n",
       "      <td>-3.642985</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.607793</td>\n",
       "      <td>-62.355423</td>\n",
       "      <td>-41.170826</td>\n",
       "      <td>0.150883</td>\n",
       "      <td>0.162260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>-2.949380</td>\n",
       "      <td>-4.846357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.896977</td>\n",
       "      <td>-80.941925</td>\n",
       "      <td>-53.032097</td>\n",
       "      <td>0.191329</td>\n",
       "      <td>0.198517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>-2.395663</td>\n",
       "      <td>-4.913164</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.517502</td>\n",
       "      <td>-76.252869</td>\n",
       "      <td>-47.093960</td>\n",
       "      <td>0.189894</td>\n",
       "      <td>0.184905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.204700</td>\n",
       "      <td>-3.467109</td>\n",
       "      <td>-5.207060</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.739951</td>\n",
       "      <td>-76.867844</td>\n",
       "      <td>-56.586136</td>\n",
       "      <td>0.170076</td>\n",
       "      <td>0.160401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>-4.934501</td>\n",
       "      <td>-6.479811</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.545310</td>\n",
       "      <td>-98.126305</td>\n",
       "      <td>-75.394524</td>\n",
       "      <td>0.176662</td>\n",
       "      <td>0.189891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>-3.315730</td>\n",
       "      <td>-6.806645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.490915</td>\n",
       "      <td>-103.007721</td>\n",
       "      <td>-57.809135</td>\n",
       "      <td>0.167934</td>\n",
       "      <td>0.186171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>-4.086520</td>\n",
       "      <td>-5.044258</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.957738</td>\n",
       "      <td>-75.606796</td>\n",
       "      <td>-64.173256</td>\n",
       "      <td>0.179583</td>\n",
       "      <td>0.187487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>-4.101552</td>\n",
       "      <td>-7.062820</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.961269</td>\n",
       "      <td>-104.560059</td>\n",
       "      <td>-62.084625</td>\n",
       "      <td>0.209035</td>\n",
       "      <td>0.249691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>-2.944464</td>\n",
       "      <td>-4.625152</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.680688</td>\n",
       "      <td>-74.154274</td>\n",
       "      <td>-56.327290</td>\n",
       "      <td>0.246309</td>\n",
       "      <td>0.242625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>-2.471912</td>\n",
       "      <td>-3.637157</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.165245</td>\n",
       "      <td>-57.070217</td>\n",
       "      <td>-45.770657</td>\n",
       "      <td>0.229133</td>\n",
       "      <td>0.223066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>-3.104383</td>\n",
       "      <td>-4.031217</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.926834</td>\n",
       "      <td>-67.120544</td>\n",
       "      <td>-58.597275</td>\n",
       "      <td>0.232756</td>\n",
       "      <td>0.218645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>-2.829027</td>\n",
       "      <td>-4.797050</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.968023</td>\n",
       "      <td>-80.608131</td>\n",
       "      <td>-48.018791</td>\n",
       "      <td>0.230919</td>\n",
       "      <td>0.230696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>-1.750187</td>\n",
       "      <td>-3.090698</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.340511</td>\n",
       "      <td>-58.243774</td>\n",
       "      <td>-36.479939</td>\n",
       "      <td>0.227629</td>\n",
       "      <td>0.215906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>-2.261680</td>\n",
       "      <td>-3.412371</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.150691</td>\n",
       "      <td>-54.097149</td>\n",
       "      <td>-41.704327</td>\n",
       "      <td>0.206764</td>\n",
       "      <td>0.213453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>-0.530094</td>\n",
       "      <td>-2.414111</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.884017</td>\n",
       "      <td>-51.580826</td>\n",
       "      <td>-25.281458</td>\n",
       "      <td>0.164165</td>\n",
       "      <td>0.167745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>-1.640215</td>\n",
       "      <td>-2.845146</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.204931</td>\n",
       "      <td>-47.136848</td>\n",
       "      <td>-34.537308</td>\n",
       "      <td>0.249346</td>\n",
       "      <td>0.270980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.567800</td>\n",
       "      <td>-1.839068</td>\n",
       "      <td>-2.508414</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.669345</td>\n",
       "      <td>-47.042782</td>\n",
       "      <td>-38.222519</td>\n",
       "      <td>0.190408</td>\n",
       "      <td>0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>-1.012042</td>\n",
       "      <td>-3.618136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.606094</td>\n",
       "      <td>-70.546066</td>\n",
       "      <td>-36.021526</td>\n",
       "      <td>0.233080</td>\n",
       "      <td>0.239975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>-2.137614</td>\n",
       "      <td>-3.194652</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.057039</td>\n",
       "      <td>-53.888687</td>\n",
       "      <td>-46.134571</td>\n",
       "      <td>0.178829</td>\n",
       "      <td>0.183813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>-1.768288</td>\n",
       "      <td>-2.860199</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.091912</td>\n",
       "      <td>-55.954567</td>\n",
       "      <td>-44.389427</td>\n",
       "      <td>0.218733</td>\n",
       "      <td>0.202064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>-1.863999</td>\n",
       "      <td>-4.078148</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.214149</td>\n",
       "      <td>-70.641159</td>\n",
       "      <td>-37.542656</td>\n",
       "      <td>0.213494</td>\n",
       "      <td>0.232668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>-2.747005</td>\n",
       "      <td>-4.854069</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.107064</td>\n",
       "      <td>-76.409286</td>\n",
       "      <td>-52.207119</td>\n",
       "      <td>0.285259</td>\n",
       "      <td>0.265351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>-2.598543</td>\n",
       "      <td>-5.108951</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.510408</td>\n",
       "      <td>-79.978973</td>\n",
       "      <td>-45.898788</td>\n",
       "      <td>0.250292</td>\n",
       "      <td>0.253725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.254700</td>\n",
       "      <td>-3.919328</td>\n",
       "      <td>-4.174363</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.255036</td>\n",
       "      <td>-65.189049</td>\n",
       "      <td>-66.408539</td>\n",
       "      <td>0.249558</td>\n",
       "      <td>0.249712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>-2.520037</td>\n",
       "      <td>-4.631811</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.111773</td>\n",
       "      <td>-76.313766</td>\n",
       "      <td>-51.290241</td>\n",
       "      <td>0.241230</td>\n",
       "      <td>0.263355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>-2.232072</td>\n",
       "      <td>-3.253336</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.021264</td>\n",
       "      <td>-58.863426</td>\n",
       "      <td>-44.435158</td>\n",
       "      <td>0.278810</td>\n",
       "      <td>0.258896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>-2.878598</td>\n",
       "      <td>-3.398767</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.520169</td>\n",
       "      <td>-56.803097</td>\n",
       "      <td>-49.959797</td>\n",
       "      <td>0.256262</td>\n",
       "      <td>0.254136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>-1.726680</td>\n",
       "      <td>-4.100198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.373518</td>\n",
       "      <td>-69.036652</td>\n",
       "      <td>-37.118042</td>\n",
       "      <td>0.244735</td>\n",
       "      <td>0.249745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>-2.395970</td>\n",
       "      <td>-3.461904</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.065934</td>\n",
       "      <td>-55.944283</td>\n",
       "      <td>-44.167412</td>\n",
       "      <td>0.203071</td>\n",
       "      <td>0.205451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>-2.006423</td>\n",
       "      <td>-3.550118</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.543695</td>\n",
       "      <td>-61.045094</td>\n",
       "      <td>-39.988503</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.246226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>-2.006008</td>\n",
       "      <td>-4.885628</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.879621</td>\n",
       "      <td>-77.207153</td>\n",
       "      <td>-43.448456</td>\n",
       "      <td>0.311765</td>\n",
       "      <td>0.298991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>-2.170492</td>\n",
       "      <td>-5.141027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.970535</td>\n",
       "      <td>-74.936508</td>\n",
       "      <td>-42.862400</td>\n",
       "      <td>0.238138</td>\n",
       "      <td>0.237484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>-3.637184</td>\n",
       "      <td>-6.535512</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.898328</td>\n",
       "      <td>-91.275658</td>\n",
       "      <td>-56.656994</td>\n",
       "      <td>0.259303</td>\n",
       "      <td>0.265299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>-4.178416</td>\n",
       "      <td>-6.766340</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.587924</td>\n",
       "      <td>-99.479729</td>\n",
       "      <td>-71.551430</td>\n",
       "      <td>0.304128</td>\n",
       "      <td>0.292525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>-3.977563</td>\n",
       "      <td>-5.384508</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.406944</td>\n",
       "      <td>-83.037354</td>\n",
       "      <td>-60.515671</td>\n",
       "      <td>0.253787</td>\n",
       "      <td>0.257593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>-3.945225</td>\n",
       "      <td>-5.542837</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.597611</td>\n",
       "      <td>-88.913933</td>\n",
       "      <td>-68.339989</td>\n",
       "      <td>0.316543</td>\n",
       "      <td>0.333835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>-2.687338</td>\n",
       "      <td>-5.232214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.544875</td>\n",
       "      <td>-83.173454</td>\n",
       "      <td>-51.676437</td>\n",
       "      <td>0.275131</td>\n",
       "      <td>0.273531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>-3.025748</td>\n",
       "      <td>-5.005919</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.980172</td>\n",
       "      <td>-74.213524</td>\n",
       "      <td>-67.301041</td>\n",
       "      <td>0.271384</td>\n",
       "      <td>0.278894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>-2.445603</td>\n",
       "      <td>-4.786144</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.340541</td>\n",
       "      <td>-71.599060</td>\n",
       "      <td>-44.647930</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>0.286157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>-2.335495</td>\n",
       "      <td>-3.812017</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.476522</td>\n",
       "      <td>-59.768887</td>\n",
       "      <td>-45.077339</td>\n",
       "      <td>0.236353</td>\n",
       "      <td>0.239097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>-4.870127</td>\n",
       "      <td>-6.914401</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2.044274</td>\n",
       "      <td>-106.286530</td>\n",
       "      <td>-78.165627</td>\n",
       "      <td>0.272372</td>\n",
       "      <td>0.304040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.833100</td>\n",
       "      <td>-3.680614</td>\n",
       "      <td>-4.363273</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.682659</td>\n",
       "      <td>-75.349907</td>\n",
       "      <td>-68.411308</td>\n",
       "      <td>0.275473</td>\n",
       "      <td>0.278180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>-4.242217</td>\n",
       "      <td>-5.183341</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.941123</td>\n",
       "      <td>-79.896454</td>\n",
       "      <td>-69.541206</td>\n",
       "      <td>0.315307</td>\n",
       "      <td>0.319164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>-2.135798</td>\n",
       "      <td>-3.428457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.292659</td>\n",
       "      <td>-61.107368</td>\n",
       "      <td>-39.299774</td>\n",
       "      <td>0.264614</td>\n",
       "      <td>0.269551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>-2.233745</td>\n",
       "      <td>-5.063753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.830009</td>\n",
       "      <td>-84.087631</td>\n",
       "      <td>-45.417587</td>\n",
       "      <td>0.261624</td>\n",
       "      <td>0.254999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>-2.815357</td>\n",
       "      <td>-5.308898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.493541</td>\n",
       "      <td>-75.515892</td>\n",
       "      <td>-47.410927</td>\n",
       "      <td>0.262382</td>\n",
       "      <td>0.261273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.260600</td>\n",
       "      <td>-6.106376</td>\n",
       "      <td>-9.410485</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.304109</td>\n",
       "      <td>-128.947418</td>\n",
       "      <td>-90.851349</td>\n",
       "      <td>0.335114</td>\n",
       "      <td>0.382127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>-2.125571</td>\n",
       "      <td>-5.490267</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.364697</td>\n",
       "      <td>-87.722893</td>\n",
       "      <td>-48.121342</td>\n",
       "      <td>0.265668</td>\n",
       "      <td>0.262861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>-3.934143</td>\n",
       "      <td>-6.819641</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.885498</td>\n",
       "      <td>-94.399567</td>\n",
       "      <td>-59.940365</td>\n",
       "      <td>0.302593</td>\n",
       "      <td>0.307267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>-3.726903</td>\n",
       "      <td>-5.597318</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.870414</td>\n",
       "      <td>-84.235931</td>\n",
       "      <td>-59.804642</td>\n",
       "      <td>0.307645</td>\n",
       "      <td>0.300347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>-2.598520</td>\n",
       "      <td>-4.739455</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.140935</td>\n",
       "      <td>-68.283630</td>\n",
       "      <td>-43.698109</td>\n",
       "      <td>0.322347</td>\n",
       "      <td>0.315986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>-2.618994</td>\n",
       "      <td>-5.309430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.690435</td>\n",
       "      <td>-78.435234</td>\n",
       "      <td>-43.818188</td>\n",
       "      <td>0.294863</td>\n",
       "      <td>0.288856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>-4.669456</td>\n",
       "      <td>-6.119825</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.450368</td>\n",
       "      <td>-87.200684</td>\n",
       "      <td>-68.600723</td>\n",
       "      <td>0.321076</td>\n",
       "      <td>0.335767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>-4.255358</td>\n",
       "      <td>-7.742585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.487226</td>\n",
       "      <td>-110.585709</td>\n",
       "      <td>-75.052139</td>\n",
       "      <td>0.307987</td>\n",
       "      <td>0.329795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>-3.867382</td>\n",
       "      <td>-7.106676</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.239294</td>\n",
       "      <td>-96.132416</td>\n",
       "      <td>-58.760250</td>\n",
       "      <td>0.327732</td>\n",
       "      <td>0.348578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>-2.994111</td>\n",
       "      <td>-6.237554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.243443</td>\n",
       "      <td>-87.598152</td>\n",
       "      <td>-47.483879</td>\n",
       "      <td>0.295236</td>\n",
       "      <td>0.286562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>-5.944111</td>\n",
       "      <td>-7.575270</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.631158</td>\n",
       "      <td>-104.501030</td>\n",
       "      <td>-83.268463</td>\n",
       "      <td>0.324033</td>\n",
       "      <td>0.318771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>-4.589965</td>\n",
       "      <td>-8.725506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.135541</td>\n",
       "      <td>-119.069397</td>\n",
       "      <td>-72.259338</td>\n",
       "      <td>0.306828</td>\n",
       "      <td>0.305175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>-3.911813</td>\n",
       "      <td>-6.483025</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.571212</td>\n",
       "      <td>-88.169495</td>\n",
       "      <td>-61.147835</td>\n",
       "      <td>0.324750</td>\n",
       "      <td>0.317110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>-3.182469</td>\n",
       "      <td>-9.240108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.057639</td>\n",
       "      <td>-122.410446</td>\n",
       "      <td>-55.928703</td>\n",
       "      <td>0.339788</td>\n",
       "      <td>0.332322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>-4.945837</td>\n",
       "      <td>-6.835369</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.889532</td>\n",
       "      <td>-91.348686</td>\n",
       "      <td>-74.127335</td>\n",
       "      <td>0.291523</td>\n",
       "      <td>0.299370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.139900</td>\n",
       "      <td>-2.949186</td>\n",
       "      <td>-7.564453</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.615267</td>\n",
       "      <td>-113.084801</td>\n",
       "      <td>-55.743408</td>\n",
       "      <td>0.321441</td>\n",
       "      <td>0.337156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>-4.616564</td>\n",
       "      <td>-7.614718</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.998154</td>\n",
       "      <td>-109.070190</td>\n",
       "      <td>-71.416046</td>\n",
       "      <td>0.330716</td>\n",
       "      <td>0.330602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>-4.487282</td>\n",
       "      <td>-7.896475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.409193</td>\n",
       "      <td>-103.919968</td>\n",
       "      <td>-64.247658</td>\n",
       "      <td>0.343931</td>\n",
       "      <td>0.352877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>-5.570286</td>\n",
       "      <td>-9.862748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.292462</td>\n",
       "      <td>-124.452255</td>\n",
       "      <td>-76.750694</td>\n",
       "      <td>0.352132</td>\n",
       "      <td>0.361623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>-5.408424</td>\n",
       "      <td>-8.877275</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.468851</td>\n",
       "      <td>-117.378929</td>\n",
       "      <td>-77.169884</td>\n",
       "      <td>0.345308</td>\n",
       "      <td>0.347575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>-5.992745</td>\n",
       "      <td>-8.994343</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.001597</td>\n",
       "      <td>-117.090271</td>\n",
       "      <td>-82.104332</td>\n",
       "      <td>0.307404</td>\n",
       "      <td>0.314249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>-5.720931</td>\n",
       "      <td>-8.168376</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.447444</td>\n",
       "      <td>-101.897667</td>\n",
       "      <td>-80.878517</td>\n",
       "      <td>0.325633</td>\n",
       "      <td>0.326783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>-3.203854</td>\n",
       "      <td>-5.530273</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.326419</td>\n",
       "      <td>-79.551170</td>\n",
       "      <td>-54.513477</td>\n",
       "      <td>0.327260</td>\n",
       "      <td>0.325576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>-3.390728</td>\n",
       "      <td>-6.750113</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.359384</td>\n",
       "      <td>-97.929626</td>\n",
       "      <td>-55.575989</td>\n",
       "      <td>0.331309</td>\n",
       "      <td>0.335961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>-4.227014</td>\n",
       "      <td>-9.046716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.819702</td>\n",
       "      <td>-121.347023</td>\n",
       "      <td>-67.834991</td>\n",
       "      <td>0.324776</td>\n",
       "      <td>0.341663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>-3.682644</td>\n",
       "      <td>-6.103726</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.421082</td>\n",
       "      <td>-90.508705</td>\n",
       "      <td>-68.662727</td>\n",
       "      <td>0.255181</td>\n",
       "      <td>0.270144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>-3.731150</td>\n",
       "      <td>-7.286363</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.555214</td>\n",
       "      <td>-100.075745</td>\n",
       "      <td>-62.195442</td>\n",
       "      <td>0.280564</td>\n",
       "      <td>0.271718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>-5.183754</td>\n",
       "      <td>-7.309748</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.125994</td>\n",
       "      <td>-98.006500</td>\n",
       "      <td>-83.482605</td>\n",
       "      <td>0.307147</td>\n",
       "      <td>0.296148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>-3.482485</td>\n",
       "      <td>-6.326893</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.844409</td>\n",
       "      <td>-87.049042</td>\n",
       "      <td>-53.715313</td>\n",
       "      <td>0.295359</td>\n",
       "      <td>0.302717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>-2.514509</td>\n",
       "      <td>-6.239382</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.724873</td>\n",
       "      <td>-85.904610</td>\n",
       "      <td>-51.101021</td>\n",
       "      <td>0.283006</td>\n",
       "      <td>0.268115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>-3.892036</td>\n",
       "      <td>-7.054849</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>3.162812</td>\n",
       "      <td>-97.405373</td>\n",
       "      <td>-68.708336</td>\n",
       "      <td>0.258867</td>\n",
       "      <td>0.248929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>-3.384413</td>\n",
       "      <td>-6.084739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.700326</td>\n",
       "      <td>-92.633942</td>\n",
       "      <td>-61.041092</td>\n",
       "      <td>0.266656</td>\n",
       "      <td>0.240447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.260600</td>\n",
       "      <td>-2.787855</td>\n",
       "      <td>-5.854146</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.066292</td>\n",
       "      <td>-90.506828</td>\n",
       "      <td>-51.708038</td>\n",
       "      <td>0.267973</td>\n",
       "      <td>0.255730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.165400</td>\n",
       "      <td>-2.758820</td>\n",
       "      <td>-5.504051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.745232</td>\n",
       "      <td>-77.002953</td>\n",
       "      <td>-45.295891</td>\n",
       "      <td>0.263319</td>\n",
       "      <td>0.270752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>-4.223430</td>\n",
       "      <td>-6.470063</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.246632</td>\n",
       "      <td>-91.893219</td>\n",
       "      <td>-63.451233</td>\n",
       "      <td>0.247194</td>\n",
       "      <td>0.252560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>-2.898420</td>\n",
       "      <td>-5.336159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.437739</td>\n",
       "      <td>-78.365631</td>\n",
       "      <td>-42.048473</td>\n",
       "      <td>0.262549</td>\n",
       "      <td>0.260788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>-3.514440</td>\n",
       "      <td>-8.417601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.903161</td>\n",
       "      <td>-117.257599</td>\n",
       "      <td>-57.074722</td>\n",
       "      <td>0.275091</td>\n",
       "      <td>0.289223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>-3.370673</td>\n",
       "      <td>-5.278588</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.907915</td>\n",
       "      <td>-78.749435</td>\n",
       "      <td>-50.455482</td>\n",
       "      <td>0.287075</td>\n",
       "      <td>0.287197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>-6.601862</td>\n",
       "      <td>-10.368532</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.766670</td>\n",
       "      <td>-136.583054</td>\n",
       "      <td>-110.721260</td>\n",
       "      <td>0.321731</td>\n",
       "      <td>0.322624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>-3.298533</td>\n",
       "      <td>-6.995568</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.697035</td>\n",
       "      <td>-98.244110</td>\n",
       "      <td>-53.021523</td>\n",
       "      <td>0.310714</td>\n",
       "      <td>0.295695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>-3.296382</td>\n",
       "      <td>-5.641071</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.344688</td>\n",
       "      <td>-82.648109</td>\n",
       "      <td>-54.357094</td>\n",
       "      <td>0.346355</td>\n",
       "      <td>0.349581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>-3.426539</td>\n",
       "      <td>-7.865748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.439210</td>\n",
       "      <td>-102.378860</td>\n",
       "      <td>-56.318047</td>\n",
       "      <td>0.284382</td>\n",
       "      <td>0.276371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>-4.766522</td>\n",
       "      <td>-8.533743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.767220</td>\n",
       "      <td>-110.798706</td>\n",
       "      <td>-75.159561</td>\n",
       "      <td>0.288154</td>\n",
       "      <td>0.287626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-4.175931</td>\n",
       "      <td>-11.708328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.532397</td>\n",
       "      <td>-153.020554</td>\n",
       "      <td>-63.048779</td>\n",
       "      <td>0.363853</td>\n",
       "      <td>0.380674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>-5.267222</td>\n",
       "      <td>-8.856168</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.588945</td>\n",
       "      <td>-117.067329</td>\n",
       "      <td>-81.623253</td>\n",
       "      <td>0.332257</td>\n",
       "      <td>0.328039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>-5.736236</td>\n",
       "      <td>-9.511347</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.775112</td>\n",
       "      <td>-118.017883</td>\n",
       "      <td>-74.155563</td>\n",
       "      <td>0.325798</td>\n",
       "      <td>0.325587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.547900</td>\n",
       "      <td>-5.435284</td>\n",
       "      <td>-7.974575</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.539291</td>\n",
       "      <td>-103.023911</td>\n",
       "      <td>-78.081917</td>\n",
       "      <td>0.281278</td>\n",
       "      <td>0.270807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.919600</td>\n",
       "      <td>-5.445876</td>\n",
       "      <td>-7.610579</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.164703</td>\n",
       "      <td>-98.156281</td>\n",
       "      <td>-75.571907</td>\n",
       "      <td>0.280482</td>\n",
       "      <td>0.275633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>-3.793734</td>\n",
       "      <td>-8.191131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.397397</td>\n",
       "      <td>-111.462051</td>\n",
       "      <td>-58.642113</td>\n",
       "      <td>0.279511</td>\n",
       "      <td>0.291262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>-4.807222</td>\n",
       "      <td>-10.135102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.327880</td>\n",
       "      <td>-136.400330</td>\n",
       "      <td>-72.047073</td>\n",
       "      <td>0.294952</td>\n",
       "      <td>0.295499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>-4.346184</td>\n",
       "      <td>-8.735615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.389430</td>\n",
       "      <td>-110.469360</td>\n",
       "      <td>-64.060081</td>\n",
       "      <td>0.255470</td>\n",
       "      <td>0.264660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>-7.190926</td>\n",
       "      <td>-14.516062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.325136</td>\n",
       "      <td>-191.103683</td>\n",
       "      <td>-102.681572</td>\n",
       "      <td>0.326447</td>\n",
       "      <td>0.358425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>-5.993521</td>\n",
       "      <td>-9.455060</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.461538</td>\n",
       "      <td>-118.638885</td>\n",
       "      <td>-87.967621</td>\n",
       "      <td>0.325028</td>\n",
       "      <td>0.322874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>-6.586555</td>\n",
       "      <td>-11.346279</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.759724</td>\n",
       "      <td>-139.262527</td>\n",
       "      <td>-90.583839</td>\n",
       "      <td>0.299623</td>\n",
       "      <td>0.299269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>-6.564862</td>\n",
       "      <td>-11.016866</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.452004</td>\n",
       "      <td>-135.049301</td>\n",
       "      <td>-88.141838</td>\n",
       "      <td>0.330869</td>\n",
       "      <td>0.349386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>-5.443419</td>\n",
       "      <td>-11.369878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.926460</td>\n",
       "      <td>-142.557953</td>\n",
       "      <td>-77.574272</td>\n",
       "      <td>0.329406</td>\n",
       "      <td>0.317719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>-9.036229</td>\n",
       "      <td>-10.813158</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.776929</td>\n",
       "      <td>-131.202774</td>\n",
       "      <td>-116.448067</td>\n",
       "      <td>0.283295</td>\n",
       "      <td>0.291994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>-6.694046</td>\n",
       "      <td>-11.196883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.502837</td>\n",
       "      <td>-142.053894</td>\n",
       "      <td>-90.434212</td>\n",
       "      <td>0.304631</td>\n",
       "      <td>0.310741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>-6.983434</td>\n",
       "      <td>-10.725935</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.742500</td>\n",
       "      <td>-135.500366</td>\n",
       "      <td>-96.452469</td>\n",
       "      <td>0.331722</td>\n",
       "      <td>0.326022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>-6.929551</td>\n",
       "      <td>-10.536995</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.607444</td>\n",
       "      <td>-132.338974</td>\n",
       "      <td>-93.276604</td>\n",
       "      <td>0.321232</td>\n",
       "      <td>0.320797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>-7.104799</td>\n",
       "      <td>-11.028508</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.923708</td>\n",
       "      <td>-133.887466</td>\n",
       "      <td>-96.153854</td>\n",
       "      <td>0.307137</td>\n",
       "      <td>0.303080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>-5.949098</td>\n",
       "      <td>-10.179416</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.230319</td>\n",
       "      <td>-136.658997</td>\n",
       "      <td>-83.493111</td>\n",
       "      <td>0.342573</td>\n",
       "      <td>0.359527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>-5.138911</td>\n",
       "      <td>-8.217067</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.078155</td>\n",
       "      <td>-108.954391</td>\n",
       "      <td>-71.924774</td>\n",
       "      <td>0.362988</td>\n",
       "      <td>0.373137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>-6.117349</td>\n",
       "      <td>-12.061934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.944584</td>\n",
       "      <td>-145.593658</td>\n",
       "      <td>-85.867722</td>\n",
       "      <td>0.282319</td>\n",
       "      <td>0.271676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>-5.320835</td>\n",
       "      <td>-11.100116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.779281</td>\n",
       "      <td>-139.121521</td>\n",
       "      <td>-75.726982</td>\n",
       "      <td>0.367161</td>\n",
       "      <td>0.368715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>-7.819239</td>\n",
       "      <td>-10.969609</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.150371</td>\n",
       "      <td>-132.056213</td>\n",
       "      <td>-99.709846</td>\n",
       "      <td>0.369808</td>\n",
       "      <td>0.374703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>-6.955665</td>\n",
       "      <td>-10.696482</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.740817</td>\n",
       "      <td>-137.637192</td>\n",
       "      <td>-92.261078</td>\n",
       "      <td>0.340401</td>\n",
       "      <td>0.335663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>-5.042141</td>\n",
       "      <td>-10.615455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.573312</td>\n",
       "      <td>-137.897217</td>\n",
       "      <td>-74.400261</td>\n",
       "      <td>0.394532</td>\n",
       "      <td>0.378773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>-8.382947</td>\n",
       "      <td>-12.539915</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.156968</td>\n",
       "      <td>-158.382935</td>\n",
       "      <td>-118.726456</td>\n",
       "      <td>0.405293</td>\n",
       "      <td>0.396588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>-5.790670</td>\n",
       "      <td>-10.565063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.774392</td>\n",
       "      <td>-139.529861</td>\n",
       "      <td>-88.790741</td>\n",
       "      <td>0.388040</td>\n",
       "      <td>0.392115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>-6.794433</td>\n",
       "      <td>-13.106553</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>6.312119</td>\n",
       "      <td>-163.546875</td>\n",
       "      <td>-90.593620</td>\n",
       "      <td>0.400730</td>\n",
       "      <td>0.410324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>-6.741181</td>\n",
       "      <td>-10.890618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.149436</td>\n",
       "      <td>-138.765686</td>\n",
       "      <td>-94.167793</td>\n",
       "      <td>0.366901</td>\n",
       "      <td>0.377439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>-4.672571</td>\n",
       "      <td>-9.999271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.326701</td>\n",
       "      <td>-129.818100</td>\n",
       "      <td>-68.540497</td>\n",
       "      <td>0.395762</td>\n",
       "      <td>0.384116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.220300</td>\n",
       "      <td>-4.830910</td>\n",
       "      <td>-7.936080</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.105169</td>\n",
       "      <td>-100.403358</td>\n",
       "      <td>-68.413940</td>\n",
       "      <td>0.392096</td>\n",
       "      <td>0.401373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>-4.440020</td>\n",
       "      <td>-10.058225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.618205</td>\n",
       "      <td>-128.479767</td>\n",
       "      <td>-66.018959</td>\n",
       "      <td>0.467281</td>\n",
       "      <td>0.448706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>-6.254722</td>\n",
       "      <td>-10.416816</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.162095</td>\n",
       "      <td>-129.057343</td>\n",
       "      <td>-83.891022</td>\n",
       "      <td>0.454070</td>\n",
       "      <td>0.458878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>-6.164343</td>\n",
       "      <td>-10.342961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.178618</td>\n",
       "      <td>-123.635773</td>\n",
       "      <td>-78.016830</td>\n",
       "      <td>0.436561</td>\n",
       "      <td>0.443459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>-8.256129</td>\n",
       "      <td>-17.818048</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>9.561920</td>\n",
       "      <td>-215.763794</td>\n",
       "      <td>-108.148079</td>\n",
       "      <td>0.499448</td>\n",
       "      <td>0.480536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>-9.867492</td>\n",
       "      <td>-14.735195</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.867703</td>\n",
       "      <td>-175.833527</td>\n",
       "      <td>-121.277138</td>\n",
       "      <td>0.496398</td>\n",
       "      <td>0.516411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    #ref_model = None,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 10,\n",
    "        learning_rate=2e-05,\n",
    "        num_train_epochs = 3,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        logging_steps = 1,\n",
    "        #optim = \"adamw_8bit\",\n",
    "        optim = 'adafactor',\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset = YOUR_DATASET_HERE,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 550,\n",
    "    model_adapter_name=\"default\",\n",
    "    ref_adapter_name=\"reference\"\n",
    ")\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa071f-22f7-4cdf-825c-21510f62e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "6*7*198213"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
